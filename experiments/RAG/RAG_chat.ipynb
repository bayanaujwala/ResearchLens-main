{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    load_index_from_storage,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    ")\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "# needed to synthesize responses later\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents\n",
    "# documents = SimpleDirectoryReader(\"/home/surya/NEU/CS5100 FAI/Project/ResearchLens/experiments/RAG/pdfs\").load_data()\n",
    "documents = SimpleDirectoryReader(\"/home/surya/NEU/CS5100 FAI/Project/ResearchLens/uploads\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# loads BAAI/bge-small-en\n",
    "# embed_model = HuggingFaceEmbedding()\n",
    "\n",
    "# loads BAAI/bge-small-en-v1.5\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.cohere import Cohere\n",
    "\n",
    "cohere_model = Cohere(api_key=\"vORtxj32na8zl2ceIbxH1c5tNziAVWDdAy2x3sbX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.embed_model = embed_model\n",
    "Settings.text_splitter = text_splitter\n",
    "Settings.llm = cohere_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-using index from RAG_doc notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, embed_model=embed_model, transformations=[text_splitter]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storage_context = StorageContext.from_defaults(\n",
    "#     persist_dir=\"./storage\"\n",
    "# )\n",
    "# index = load_index_from_storage(storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=1500)\n",
    "\n",
    "chat_engine = index.as_chat_engine(\n",
    "    chat_mode=\"context\",\n",
    "    memory=memory,\n",
    "    llm=cohere_model,\n",
    "    context_prompt=(\n",
    "        \"You are a chatbot, able to have normal interactions, as well as explaining research papers.\"\n",
    "        \"Here are the relevant documents for the context:\\n\"\n",
    "        \"{context_str}\"\n",
    "        \"\\nInstruction: Use the previous chat history, or the context above, to interact and help the user.\"\n",
    "    ),\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat_engine = index.as_chat_engine(\n",
    "#     chat_mode=\"condense_plus_context\",\n",
    "#     context_prompt=(\n",
    "#         \"You are a chatbot, able to have normal interactions, as well as talk\"\n",
    "#         \" about an essay discussing Paul Grahams life.\"\n",
    "#         \"Here are the relevant documents for the context:\\n\"\n",
    "#         \"{context_str}\"\n",
    "#         \"\\nInstruction: Based on the above documents, provide a detailed answer for the user question below.\"\n",
    "#     ),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_engine.chat(\"\"\"\n",
    "On single-turn dialogue, we evaluate the different methods on the subset of the test split of the\n",
    "Anthropic HH dataset [1] with one step of human-assistant interaction. GPT-4 evaluations use the\n",
    "preferred completions on the test as the reference to compute the win rate for different methods. As\n",
    "there is no standard SFT model for this task, we start with a pre-trained Pythia-2.8B, use Preferred-FT\n",
    "to train a reference model on the chosen completions such that completions are within distribution\n",
    "of the model, and then train using DPO. We also compare against the best of 128 Preferred-FT\n",
    "completions (we found the Best of N baseline plateaus at 128 completions for this task; see Appendix\n",
    "Figure 4) and a 2-shot prompted version of the Pythia-2.8B base model, finding DPO performs as\n",
    "well or better for the best-performing temperatures for each method.\n",
    "\n",
    "What dataset was used to evaluate the RLHF Model?       \n",
    "\"\"\".strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response.source_nodes[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for chat in chat_engine.chat_history:\n",
    "#     print(chat.role)\n",
    "#     print(chat.content)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
