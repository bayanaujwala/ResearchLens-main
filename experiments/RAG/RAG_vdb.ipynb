{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# import sys\n",
    "\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    load_index_from_storage,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    ")\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "# needed to synthesize responses later\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents\n",
    "documents = SimpleDirectoryReader(\"/home/surya/NEU/CS5100 FAI/Project/ResearchLens/uploads\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# loads BAAI/bge-small-en\n",
    "# embed_model = HuggingFaceEmbedding()\n",
    "\n",
    "# loads BAAI/bge-small-en-v1.5\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.embed_model = embed_model\n",
    "Settings.text_splitter = text_splitter\n",
    "Settings.llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, embed_model=embed_model, transformations=[text_splitter]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "disk away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist(\"./storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save index to disk\n",
    "# index.storage_context.persist(\"./storage\")\n",
    "\n",
    "# storage_context = StorageContext.from_defaults(\n",
    "#     persist_dir=\"./storage\"\n",
    "# )\n",
    "# index = load_index_from_storage(storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add a new document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['docstore/metadata', 'docstore/data', 'docstore/ref_doc_info'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.docstore.to_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'None'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHeck doc exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_exist(index, filepath):\n",
    "    for node in index.docstore.docs.values():\n",
    "        if node.metadata['file_path'] == filepath:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_exist(index, '/home/surya/NEU/CS5100 FAI/Project/ResearchLens/uploads/test4.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index.docstore.docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file  = \"/home/surya/NEU/CS5100 FAI/Project/pdfreader/test2.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(input_files=[pdf_file]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_chunks = []\n",
    "\n",
    "# Iterate over each document's text\n",
    "for document in documents:\n",
    "    # Assume each document has a text attribute containing its content\n",
    "    document_text = document.text\n",
    "    \n",
    "    # Split the document's text into chunks\n",
    "    chunks = text_splitter.split_text(document_text)\n",
    "    \n",
    "    # Add the chunks from this document to the list of all chunks\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "len(all_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of documents after inserting these 27 document chunks must be:\n",
    "\n",
    "67 + 85 = 152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in documents:\n",
    "    index.insert(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index.docstore.docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "close enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine = index.as_chat_engine(\n",
    "    chat_mode=\"condense_plus_context\",\n",
    "    context_prompt=(\n",
    "        \"You are a chatbot, able to have normal interactions, as well as talk\"\n",
    "        \" about an essay discussing Paul Grahams life.\"\n",
    "        \"Here are the relevant documents for the context:\\n\"\n",
    "        \"{context_str}\"\n",
    "        \"\\nInstruction: Based on the above documents, provide a detailed answer for the user question below.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_engine.chat(\"\"\"\n",
    "At a high level, existing methods instill the desired behaviors into a language model using curated\n",
    "sets of human preferences representing the types of behaviors that humans find safe and helpful. This\n",
    "preference learning stage occurs after an initial stage of large-scale unsupervised pre-training on\n",
    "a large text dataset. While the most straightforward approach to preference learning is supervised\n",
    "fine-tuning on human demonstrations of high quality responses, the most successful class of methods\n",
    "is reinforcement learning from human (or AI) feedback (RLHF/RLAIF; [12, 2]). \n",
    "\n",
    "What does the RLHF model do?\n",
    "\"\"\".strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system: You are a chatbot, able to have normal interactions, as well as talk about an essay discussing Paul Grahams life.Here are the relevant documents for the context:\n",
      "page_label: 2\n",
      "file_path: /home/surya/NEU/CS5100 FAI/Project/pdfreader/test2.pdf\n",
      "\n",
      "Figure 1: DPO optimizes for human preferences while avoiding reinforcement learning. Existing methods\n",
      "for fine-tuning language models with human feedback first fit a reward model to a dataset of prompts and\n",
      "human preferences over pairs of responses, and then use RL to find a policy that maximizes the learned reward.\n",
      "In contrast, DPO directly optimizes for the policy best satisfying the preferences with a simple classification\n",
      "objective, fitting an implicit reward model whose corresponding optimal policy can be extracted in closed form.\n",
      "we will show that the RL-based objective used by existing methods can be optimized exactly with a\n",
      "simple binary cross-entropy objective, greatly simplifying the preference learning pipeline.\n",
      "At a high level, existing methods instill the desired behaviors into a language model using curated\n",
      "sets of human preferences representing the types of behaviors that humans find safe and helpful. This\n",
      "preference learning stage occurs after an initial stage of large-scale unsupervised pre-training on\n",
      "a large text dataset. While the most straightforward approach to preference learning is supervised\n",
      "fine-tuning on human demonstrations of high quality responses, the most successful class of methods\n",
      "is reinforcement learning from human (or AI) feedback (RLHF/RLAIF; [ 12,2]). RLHF methods fit\n",
      "a reward model to a dataset of human preferences and then use RL to optimize a language model\n",
      "policy to produce responses assigned high reward without drifting excessively far from the original\n",
      "model. While RLHF produces models with impressive conversational and coding abilities, the RLHF\n",
      "pipeline is considerably more complex than supervised learning, involving training multiple LMs and\n",
      "sampling from the LM policy in the loop of training, incurring significant computational costs.\n",
      "In this paper, we show how to directly optimize a language model to adhere to human preferences,\n",
      "without explicit reward modeling or reinforcement learning. We propose Direct Preference Optimiza-\n",
      "tion (DPO) , an algorithm that implicitly optimizes the same objective as existing RLHF algorithms\n",
      "(reward maximization with a KL-divergence constraint) but is simple to implement and straight-\n",
      "forward to train. Intuitively, the DPO update increases the relative log probability of preferred to\n",
      "dispreferred responses, but it incorporates a dynamic, per-example importance weight that prevents\n",
      "the model degeneration that we find occurs with a naive probability ratio objective.\n",
      "\n",
      "page_label: 1\n",
      "file_path: /home/surya/NEU/CS5100 FAI/Project/pdfreader/test2.pdf\n",
      "\n",
      "Direct Preference Optimization:\n",
      "Your Language Model is Secretly a Reward Model\n",
      "Rafael Rafailov∗†Archit Sharma∗†Eric Mitchell∗†\n",
      "Stefano Ermon†‡Christopher D. Manning†Chelsea Finn†\n",
      "†Stanford University‡CZ Biohub\n",
      "{rafailov,architsh,eric.mitchell}@cs.stanford.edu\n",
      "Abstract\n",
      "While large-scale unsupervised language models (LMs) learn broad world knowl-\n",
      "edge and some reasoning skills, achieving precise control of their behavior is\n",
      "difficult due to the completely unsupervised nature of their training. Existing\n",
      "methods for gaining such steerability collect human labels of the relative quality of\n",
      "model generations and fine-tune the unsupervised LM to align with these prefer-\n",
      "ences, often with reinforcement learning from human feedback (RLHF). However,\n",
      "RLHF is a complex and often unstable procedure, first fitting a reward model that\n",
      "reflects the human preferences, and then fine-tuning the large unsupervised LM\n",
      "using reinforcement learning to maximize this estimated reward without drifting\n",
      "too far from the original model. In this paper we introduce a new parameterization\n",
      "of the reward model in RLHF that enables extraction of the corresponding optimal\n",
      "policy in closed form, allowing us to solve the standard RLHF problem with only a\n",
      "simple classification loss. The resulting algorithm, which we call Direct Prefer-\n",
      "ence Optimization (DPO), is stable, performant, and computationally lightweight,\n",
      "eliminating the need for sampling from the LM during fine-tuning or performing\n",
      "significant hyperparameter tuning. Our experiments show that DPO can fine-tune\n",
      "LMs to align with human preferences as well as or better than existing methods.\n",
      "Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sen-\n",
      "timent of generations, and matches or improves response quality in summarization\n",
      "and single-turn dialogue while being substantially simpler to implement and train.\n",
      "1 Introduction\n",
      "Large unsupervised language models (LMs) trained on very large datasets acquire surprising capabili-\n",
      "ties [ 11,7,40,8]. However, these models are trained on data generated by humans with a wide variety\n",
      "of goals, priorities, and skillsets.\n",
      "Instruction: Based on the above documents, provide a detailed answer for the user question below.\n",
      "user: At a high level, existing methods instill the desired behaviors into a language model using curated\n",
      "sets of human preferences representing the types of behaviors that humans find safe and helpful. This\n",
      "preference learning stage occurs after an initial stage of large-scale unsupervised pre-training on\n",
      "a large text dataset. While the most straightforward approach to preference learning is supervised\n",
      "fine-tuning on human demonstrations of high quality responses, the most successful class of methods\n",
      "is reinforcement learning from human (or AI) feedback (RLHF/RLAIF; [12, 2]). \n",
      "\n",
      "What does the RLHF model do?\n",
      "assistant: \n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 1: DPO optimizes for human preferences while avoiding reinforcement learning. Existing methods\n",
      "for fine-tuning language models with human feedback first fit a reward model to a dataset of prompts and\n",
      "human preferences over pairs of responses, and then use RL to find a policy that maximizes the learned reward.\n",
      "In contrast, DPO directly optimizes for the policy best satisfying the preferences with a simple classification\n",
      "objective, fitting an implicit reward model whose corresponding optimal policy can be extracted in closed form.\n",
      "we will show that the RL-based objective used by existing methods can be optimized exactly with a\n",
      "simple binary cross-entropy objective, greatly simplifying the preference learning pipeline.\n",
      "At a high level, existing methods instill the desired behaviors into a language model using curated\n",
      "sets of human preferences representing the types of behaviors that humans find safe and helpful. This\n",
      "preference learning stage occurs after an initial stage of large-scale unsupervised pre-training on\n",
      "a large text dataset. While the most straightforward approach to preference learning is supervised\n",
      "fine-tuning on human demonstrations of high quality responses, the most successful class of methods\n",
      "is reinforcement learning from human (or AI) feedback (RLHF/RLAIF; [ 12,2]). RLHF methods fit\n",
      "a reward model to a dataset of human preferences and then use RL to optimize a language model\n",
      "policy to produce responses assigned high reward without drifting excessively far from the original\n",
      "model. While RLHF produces models with impressive conversational and coding abilities, the RLHF\n",
      "pipeline is considerably more complex than supervised learning, involving training multiple LMs and\n",
      "sampling from the LM policy in the loop of training, incurring significant computational costs.\n",
      "In this paper, we show how to directly optimize a language model to adhere to human preferences,\n",
      "without explicit reward modeling or reinforcement learning. We propose Direct Preference Optimiza-\n",
      "tion (DPO) , an algorithm that implicitly optimizes the same objective as existing RLHF algorithms\n",
      "(reward maximization with a KL-divergence constraint) but is simple to implement and straight-\n",
      "forward to train. Intuitively, the DPO update increases the relative log probability of preferred to\n",
      "dispreferred responses, but it incorporates a dynamic, per-example importance weight that prevents\n",
      "the model degeneration that we find occurs with a naive probability ratio objective.\n"
     ]
    }
   ],
   "source": [
    "print(response.source_nodes[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
