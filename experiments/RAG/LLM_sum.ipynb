{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    ")\n",
    "from llama_index.embeddings.cohere import CohereEmbedding\n",
    "from llama_index.llms.cohere import Cohere\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "# needed to synthesize responses later\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = CohereEmbedding(\n",
    "        cohere_api_key=\"vORtxj32na8zl2ceIbxH1c5tNziAVWDdAy2x3sbX\",\n",
    "        model_name=\"embed-english-v3.0\", # Supports all Cohere embed models\n",
    "        input_type=\"search_query\", # Required for v3 models\n",
    "    )\n",
    "\n",
    "llm_model = Cohere(\n",
    "    model='command-r',\n",
    "    api_key=\"vORtxj32na8zl2ceIbxH1c5tNziAVWDdAy2x3sbX\",\n",
    "    temperature=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR= \"/home/surya/NEU/CS5100 FAI/Project/ResearchLens/uploads\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_pdf = os.path.join(DIR, \"2311.17902.pdf\")\n",
    "citing_pdf = os.path.join(DIR, \"2201.02605.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(input_files=[citing_pdf]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents, embed_model=embed_model, transformations=[text_splitter]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "co = cohere.Client(\"vORtxj32na8zl2ceIbxH1c5tNziAVWDdAy2x3sbX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed all documents to the chat\n",
    "\n",
    "Let's ask something specific from the later pages to verify it is working accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_docs = []\n",
    "\n",
    "# for doc in documents:\n",
    "#     if doc.metadata['file_path'] == citing_pdf:\n",
    "#         selected_docs.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate token length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10931, 8198.25)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = 0\n",
    "\n",
    "for doc in documents:\n",
    "    words += len(doc.text.split())\n",
    "\n",
    "words, words * 3/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "Similar to DECOLA Phase 2, we self-\n",
    "train baseline on weakly-labeled data. For the self-training\n",
    "algorithm, we use online self-training with max-size loss\n",
    "from Detic [74] as baseline comparison (baseline + self-\n",
    "train) to DECOLA Phase 2. We tested max-size and max-\n",
    "score losses from Detic [74] (online pseudo-labeling) as well\n",
    "as offline pseudo-labeling similar to DECOLA , and max-size\n",
    "loss consistently performed the best.\n",
    "\n",
    "What is the Detic model mentioned in this paragraph? What are the novel contributions of the Detic model?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_doc_text = \"\\n\".join([doc.text for doc in documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Given the following question about a paper from the user, please provide a brief compilation of texts from the referring paper that can help answer the question. \n",
    "\n",
    "Question: \n",
    "{question}\n",
    "\n",
    "Referring Paper Content:\n",
    "{full_doc_text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Do not rephrase the output text, simply provide the relevant text snippets from the referring paper.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given the following question about a paper from the user, please provide a brief compilation of texts from the referring paper that can help answer the question. \n",
      "\n",
      "Question: \n",
      "\n",
      "Similar to DECOLA Phase 2, we self-\n",
      "train baseline on weakly-labeled data. For the self-training\n",
      "algorithm, we use online self-training with max-size loss\n",
      "from Detic [74] as baseline comparison (baseline + self-\n",
      "train) to DECOLA Phase 2. We tested max-size and max-\n",
      "score losses from Detic [74] (online pseudo-labeling) as well\n",
      "as offline pseudo-labeling similar to DECOLA , and max-size\n",
      "loss consistently performed the best.\n",
      "\n",
      "What is the Detic model mentioned in this paragraph? What are the novel contributions of the Detic model?\n",
      "\n",
      "\n",
      "Referring Paper Content:\n",
      "Detecting Twenty-thousand Classes\n",
      "using Image-level Supervision\n",
      "Xingyi Zhou1,2⋆Rohit Girdhar1Armand Joulin1\n",
      "Philipp Kr¨ ahenb¨ uhl2Ishan Misra1\n",
      "1Meta AI2The University of Texas at Austin\n",
      "Abstract. Current object detectors are limited in vocabulary size due to\n",
      "the small scale of detection datasets. Image classiﬁers, on the other hand,\n",
      "reason about much larger vocabularies, as their datasets are larger and\n",
      "easier to collect. We propose Detic , which simply trains the classiﬁers of a\n",
      "detector on image classiﬁcation data and thus expands the vocabulary of\n",
      "detectors to tens of thousands of concepts. Unlike prior work, Detic does\n",
      "not need complex assignment schemes to assign image labels to boxes\n",
      "based on model predictions, making it much easier to implement and\n",
      "compatible with a range of detection architectures and backbones. Our\n",
      "results show that Detic yields excellent detectors even for classes without\n",
      "box annotations. It outperforms prior work on both open-vocabulary and\n",
      "long-tail detection benchmarks. Detic provides a gain of 2.4 mAP for\n",
      "all classes and 8.3 mAP for novel classes on the open-vocabulary LVIS\n",
      "benchmark. On the standard LVIS benchmark, Detic obtains 41.7 mAP\n",
      "when evaluated on all classes, or only rare classes, hence closing the gap in\n",
      "performance for object categories with few samples. For the ﬁrst time, we\n",
      "train a detector with all the twenty-one-thousand classes of the ImageNet\n",
      "dataset and show that it generalizes to new datasets without ﬁnetuning.\n",
      "Code is available at https://github.com/facebookresearch/Detic .\n",
      "1 Introduction\n",
      "Object detection consists of two sub-problems - ﬁnding the object (localization)\n",
      "and naming it (classiﬁcation). Traditional methods tightly couple these two sub-\n",
      "problems and thus rely on box labels for all classes. Despite many data collection\n",
      "eﬀorts, detection datasets [ 18,28,34,49] are much smaller in overall size and\n",
      "vocabularies than classiﬁcation datasets [ 10]. For example, the recent LVIS\n",
      "detection dataset [ 18] has 1000+ classes with 120K images; OpenImages [ 28] has\n",
      "500 classes in 1.8M images. Moreover, not all classes contain suﬃcient annotations\n",
      "to train a robust detector (see Figure 1 Top). In classiﬁcation, even the ten-year-\n",
      "old ImageNet [10] has 21K classes and 14M images (Figure 1 Bottom).\n",
      "In this paper, we propose Detector with image classes (Detic) that uses\n",
      "image-level supervision in addition to detection supervision. We observe that the\n",
      "localization and classiﬁcation sub-problems can be decoupled. Modern region\n",
      "⋆Work done during an internship at Meta.arXiv:2201.02605v3  [cs.CV]  29 Jul 2022\n",
      "2 X. Zhou et al.\n",
      "carcarbearotter, brown bear\n",
      "5001000150020002500\n",
      "CarUnicycleFoxOtterXboXCoﬀee-\n",
      "cupOnly image-level labelsConceptual CaptionsImageNetLVIS\n",
      "0Number of imagesStamperViolaPaper-\n",
      "clipPigLion\n",
      "carcarbearotter, brown bear\n",
      "5001000150020002500\n",
      "CarUnicycleFoxOtterXboXCoﬀee-\n",
      "cupOnly image-level labelsConceptual CaptionsImageNetLVIS\n",
      "0Number of imagesStamperViolaPaper-\n",
      "clipPigLion\n",
      "Fig. 1: Top: Typical detection results from a strong open-vocabulary LVIS detector.\n",
      "The detector misses objects of “common” classes. Bottom: Number of images in LVIS,\n",
      "ImageNet, and Conceptual Captions per class (smoothed by averaging 100 neighboring\n",
      "classes). Classiﬁcation datasets have a much larger vocabulary than detection datasets.\n",
      "proposal networks already localize many ‘new’ objects using existing detection\n",
      "supervision. Thus, we focus on the classiﬁcation sub-problem and use image-level\n",
      "labels to train the classiﬁer and broaden the vocabulary of the detector. We\n",
      "propose a simple classiﬁcation loss that applies the image-level supervision to\n",
      "the proposal with the largest size, and do not supervise other outputs for image-\n",
      "labeled data. This is easy to implement and massively expands the vocabulary.\n",
      "Most existing weakly-supervised detection techniques [ 13,22,36,59,67] use\n",
      "the weakly labeled data to supervise both the localization and classiﬁcation\n",
      "sub-problems of detection. Since image-classiﬁcation data has no box labels,\n",
      "these methods develop various label-to-box assignment techniques based on model\n",
      "predictions to obtain supervision. For example, YOLO9000[ 45] and DLWL[ 44]\n",
      "assign the image label to proposals that have high prediction scores on the\n",
      "labeled class. Unfortunately, this prediction-based assignment requires good initial\n",
      "detections which leads to a chicken-and-egg problem—we need a good detector\n",
      "for good label assignment, but we need many boxes to train a good detector. Our\n",
      "method completely side-steps the prediction-based label assignment process by\n",
      "supervising the classiﬁcation sub-problem alone when using classiﬁcation data.\n",
      "This also enables our method to learn detectors for new classes which would have\n",
      "been impossible to predict and assign.\n",
      "Experiments on the open-vocabulary LVIS [ 17,18] and the open-vocabulary\n",
      "COCO [ 2] benchmarks show that our method can signiﬁcantly improve over\n",
      "a strong box-supervised baseline, on both novel and base classes. With image-\n",
      "level supervision from ImageNet-21K [ 10], our model trained without novel\n",
      "class detection annotations improves the baseline by 8 .3 point and matches the\n",
      "performance of using full class annotations in training. With the standard LVIS\n",
      "annotations, our model reaches 41 .7 mAP and 41 .7 mAP rare, closing the gap\n",
      "between rare classes and all classes. On open-vocabulary COCO, our method\n",
      "Detecting Twenty-thousand Classes 3\n",
      "Sports\tballPersonPersonPerson\n",
      "(a)Standard detection\n",
      "PersonSports\tball\n",
      "? (b)Prediction-based label assignment\n",
      "PersonSports\tball\n",
      " (c)Our non-prediction-based loss\n",
      "Fig. 2: Left: Standard detection requires ground-truth labeled boxes and cannot lever-\n",
      "age image-level labels. Center: Existing prediction-based weakly supervised detection\n",
      "methods [ 3,44,45] use image-level labels by assigning them to the detector’s predicted\n",
      "boxes (proposals). Unfortunately, this assignment is error-prone, especially for large\n",
      "vocabulary detection. Right: Detic simply assigns the image-labels to the max-size\n",
      "proposal. We show that this loss is both simpler and performs better than prior work.\n",
      "outperforms the previous state-of-the-art OVR-CNN [ 72] by 5 point with the\n",
      "same detector and data. Finally, we train a detector using the full ImageNet-\n",
      "21K with more than twenty-thousand classes. Our detector generalizes much\n",
      "better to new datasets [ 28,49] with disjoint label spaces, reaching 21 .5 mAP on\n",
      "Objects365 and 55 .2 mAP50 on OpenImages, without seeing any images from\n",
      "the corresponding training sets. Our contributions are summarized below:\n",
      "–We identify issues and propose a simpler alternative to existing weakly-\n",
      "supervised detection techniques in the open-vocabulary setting.\n",
      "–Our proposed family of losses signiﬁcantly improves detection performance\n",
      "on novel classes, closely matching the supervised performance upper bound.\n",
      "–Our detector transfers to new datasets and vocabularies without ﬁnetuning.\n",
      "–We release our code (in supplement). It is ready-to-use for open-vocabulary\n",
      "detection in the real world. See examples in supplement.\n",
      "2 Related Work\n",
      "Weakly-supervised object detection (WSOD) trains object detector using\n",
      "image-level labels. Many works use only image-level labels without any box\n",
      "supervision [ 30,51,52,63,70]. WSDDN [ 3] and OIRC [ 60] use a subnetwork to\n",
      "predict per-proposal weighting and sum up proposal scores into a single image\n",
      "scores. PCL [ 59] ﬁrst clusters proposals and then assign image labels at the cluster\n",
      "level. CASD [ 22] further introduces feature-level attention and self-distillation. As\n",
      "no bounding box supervision is used in training, these methods rely on low-level\n",
      "region proposal techniques [1, 62], which leads to reduced localization quality.\n",
      "Another line of WSOD work uses bounding box supervision together with\n",
      "image labels, known as semi-supervised WSOD [12,13,31,35,61,68,75].\n",
      "YOLO9000 [ 45] mixes detection data and classiﬁcation data in the same mini-\n",
      "batch, and assigns classiﬁcation labels to anchors with the highest predicted scores.\n",
      "DLWL [ 44] combines self-training and clustering-based WSOD [ 59], and again\n",
      "assigns image labels to max-scored proposals. MosaicOS [ 73] handles domain\n",
      "diﬀerences between detection and image datasets by mosaic augmentation [ 4]\n",
      "and proposed a three-stage self-training and ﬁnetuning framework. In segmenta-\n",
      "tion, Pinheiro et al. [41] use a log-sum-exponential function to aggregate pixels\n",
      "4 X. Zhou et al.\n",
      "scores into a global classiﬁcation. Our work belongs to semi-supervised WSOD.\n",
      "Unlike prior work, we use a simple image-supervised loss. Besides image la-\n",
      "bels, researchers have also studied complementary methods for weak localization\n",
      "supervision like points [7] or scribles [47].\n",
      "Open-vocabulary object detection , or also named zero-shot object de-\n",
      "tection , aims to detect objects outside of the training vocabulary. The basic\n",
      "solution [ 2] is to replace the last classiﬁcation layer with language embeddings\n",
      "(e.g., GloVe [ 40]) of the class names. Rahman et al. [43] and Li et al. [33] improve\n",
      "the classiﬁer embedding using external text information. OVR-CNN [ 72] pretrains\n",
      "the detector on image-text pairs. ViLD [ 17], OpenSeg [ 16] and langSeg [ 29] up-\n",
      "grade the language embedding to CLIP [ 42]. ViLD further distills region features\n",
      "from CLIP image features. We use CLIP [ 42] classiﬁer as well, but do not use\n",
      "distillation. Instead, we use additional image-labeled data for co-training.\n",
      "Large-vocabulary object detection [18,45,53,69] requires detecting 1000+\n",
      "classes. Many existing works focus on handling the long-tail problem [ 6,14,\n",
      "32,39,65,74]. Equalization losses [ 55,56] and SeeSaw loss [ 64] reweights the\n",
      "per-class loss by balancing the gradients [ 55] or number of samples [ 64]. Federated\n",
      "Loss [ 76] subsamples classes per-iteration to mimic the federated annotation [ 18].\n",
      "Yang et al. [69] detects 11K classes with a label hierarchy. Our method builds\n",
      "on these advances, and we tackle the problem from a diﬀerent aspect: using\n",
      "additional image-labeled data.\n",
      "Proposal Network Generalization. ViLD [ 17] reports that region proposal\n",
      "networks have certain generalization abilities for new classes by default. Dave\n",
      "et al. [9] shows segmentation and localization generalizes across classes. Kim\n",
      "et al. [25] further improves proposal generalization with a localization quality\n",
      "estimator. In our experiments, we found proposals to generalize well enough\n",
      "(see Appendix A), as also observed in ViLD [ 17]. Further improvements to\n",
      "RPNs [17, 25, 27, 38] can hopefully lead to better results.\n",
      "3 Preliminaries\n",
      "We train object detectors using both object detection and image classiﬁcation\n",
      "datasets. We propose a simple way to leverage image supervision to learn object\n",
      "detectors, including for classes without box labels. We ﬁrst describe the object\n",
      "detection problem and then detail our approach.\n",
      "Problem setup. Given an image I∈R3×h×w, object detection solves the two\n",
      "subproblems of (1) localization: ﬁnd all objects with their location, represented\n",
      "as a box bj∈R4and (2) classiﬁcation: assign a class label cj∈Ctestto thej-th\n",
      "object. HereCtestis the class vocabulary provided by the user at test time. During\n",
      "training, we use a detection dataset Ddet={(I,{(b,c)k})i}|Ddet|\n",
      "i=1 with vocabulary\n",
      "Cdetthat has both class and box labels. We also use an image classiﬁcation\n",
      "datasetDcls={(I,{ck})i}|Dcls|\n",
      "i=1with vocabulary Cclsthat only has image-level\n",
      "class labels. The vocabularies Ctest,Cdet,Cclsmay or may not overlap.\n",
      "Traditional Object detection considersCtest=CdetandDcls=∅. Predom-\n",
      "inant object detectors [ 20,46] follow a two-stage framework. The ﬁrst stage,\n",
      "Detecting Twenty-thousand Classes 5\n",
      "Proposals…PersonSports\tballBackground\n",
      "Detector{fj}W\n",
      "B\n",
      "LionMax-size\tproposalDetectorWfj\n",
      "(a)Detection data\n",
      "Proposals…PersonSports\tballBackground\n",
      "Detector{fj}W\n",
      "B\n",
      "LionMax-size\tproposalDetectorWfj (b)Image-labeled data\n",
      "Fig. 3: Approach Overview. We mix train on detection data and image-labeled\n",
      "data. When using detection data, our model uses the standard detection losses to\n",
      "train the classiﬁer ( W) and the box prediction branch ( B) of a detector. When using\n",
      "image-labeled data, we only train the classiﬁer using our modiﬁed classiﬁcation loss.\n",
      "Our loss trains the features extracted from the largest-sized proposal.\n",
      "called the region proposal network (RPN), takes the image Iand produces a set\n",
      "of object proposals {(b,f,o)j}, where fj∈RDis aD-dimensional region feature\n",
      "ando∈Ris the objectness score. The second stage takes the object feature and\n",
      "outputs a classiﬁcation score and a reﬁned box location for each object, sj=Wfj,\n",
      "ˆbj=Bfj+bj, where W∈R|Cdet|×DandB∈R4×Dare the learned weights of\n",
      "the classiﬁcation layer and the regression layer, respectively.1Our work focuses\n",
      "on improving classiﬁcation in the second stage. In our experiments, the proposal\n",
      "network and the bounding box regressors are not the current performance bottle-\n",
      "neck, as modern detectors use an over-suﬃcient number of proposals in testing\n",
      "(1K proposals for <20 objects per image. see Appendix A for more details).\n",
      "Open-vocabulary object detection allowsCtest̸=Cdet. Simply replacing the\n",
      "classiﬁcation weights Wwith ﬁxed language embeddings of class names converts\n",
      "a traditional detector to an open-vocabulary detector [ 2]. The region features\n",
      "are trained to match the ﬁxed language embeddings. We follow Gu et al. [17]\n",
      "to use the CLIP embeddings [ 42] as the classiﬁcation weights. In theory, this\n",
      "open-vocabulary detector can detect any object class. However, in practice, it\n",
      "yields unsatisfying results as shown in Figure 1. Our method uses image-level\n",
      "supervision to improve object detection including in the open-vocabulary setting.\n",
      "4 Detic: Detector with Image Classes\n",
      "As shown in Figure 3, our method leverages the box labels from detection datasets\n",
      "Ddetand image-level labels from classiﬁcation datasets Dcls. During training,\n",
      "we compose a mini-batch using images from both types of datasets. For images\n",
      "with box labels, we follow the standard two-stage detector training [ 46]. For\n",
      "image-level labeled images, we only train the features from a ﬁxed region proposal\n",
      "for classiﬁcation. Thus, we only compute the localization losses (RPN loss and\n",
      "bounding box regression loss) on images with ground truth box labels. Below we\n",
      "describe our modiﬁed classiﬁcation loss for image-level labels.\n",
      "A sample from the weakly labeled dataset Dclscontains an image Iand a set of\n",
      "Klabels{ck}K\n",
      "k=1. We use the region proposal network to extract Nobject features\n",
      "{(b,f,o)j}N\n",
      "j=1. Prediction-based methods try to assign image labels to regions,\n",
      "1We omit the two linear layers and the bias in the second stage for notation simplicity.\n",
      "6 X. Zhou et al.\n",
      "and aim to train both localization and classiﬁcation abilities. Instead, we propose\n",
      "simple ways to use the image labels {ck}K\n",
      "k=1and only improve classiﬁcation. Our\n",
      "key idea is to use a ﬁxed way to assign image labels to regions, and side-step a\n",
      "complex prediction-based assignment. We allow the ﬁxed assignment schemes\n",
      "miss certain objects, as long as they miss fewer objects than the prediction-based\n",
      "counterparts, thus leading to better performance.\n",
      "Non-prediction-based losses. We now describe a variety of simple ways to\n",
      "use image labels and evaluate them empirically in Table 1. Our ﬁrst idea is to\n",
      "use the whole image as a new “proposal” box. We call this loss image-box . We\n",
      "ignore all proposals from the RPN, and instead use an injected box of the whole\n",
      "image b′= (0,0,w,h ). We then apply the classiﬁcation loss to its RoI features f′\n",
      "for all classes c∈{ck}K\n",
      "k=1:\n",
      "Limage-box =BCE (Wf′,c)\n",
      "whereBCE (s,c) =−logσ(sc)−∑\n",
      "k̸=clog(1−σ(sk)) is the binary cross-entropy\n",
      "loss, andσis the sigmoid activation. Thus, our loss uses the features from the\n",
      "same ‘proposal’ for solving the classiﬁcation problem for all the classes {ck}.\n",
      "In practice, the image-box can be replaced by smaller boxes. We introduce\n",
      "two alternatives: the proposal with the max object score or the proposal with\n",
      "themax size :\n",
      "Lmax-object-score =BCE (Wfj,c),j= argmaxjoj\n",
      "Lmax-size =BCE (Wfj,c),j= argmaxj(size(bj))\n",
      "We show that all these three losses can eﬀectively leverage the image-level\n",
      "supervision, while the max-size loss performs the best. We thus use the max-size\n",
      "loss by default for image-supervised data. We also note that the classiﬁcation\n",
      "parameters Ware shared across both detection and classiﬁcation data, which\n",
      "greatly improves detection performance. The overall training objective is\n",
      "L(I) ={\n",
      "Lrpn+Lreg+Lcls,ifI∈Ddet\n",
      "λLmax-size, ifI∈Dcls\n",
      "whereLrpn,Lreg,Lclsare standard losses in a two-stage detector, and λ= 0.1 is\n",
      "the weight of our loss.\n",
      "Relation to prediction-based assignments. In traditional weakly-supervised\n",
      "detection [ 3,44,45], a popular idea is to assign the image to the proposals\n",
      "based on model prediction. Let F= (f1,...,fN) be the stacked feature of all\n",
      "object proposals and S=WF be their classiﬁcation scores. For each c∈{ck}K\n",
      "k=1,\n",
      "L=BCE (Sj,c),j=F(S,c), whereFis the label-to-box assignment process. In\n",
      "most methods,Fis a function of the prediction S. For example,Fselects the\n",
      "proposal with max score on c. Our key insight is that Fshould notdepend on\n",
      "the prediction S. In large-vocabulary detection, the initial recognition ability of\n",
      "rare or novel classes is low, making the label assignment process inaccurate. Our\n",
      "method side-steps this prediction-and-assignment process entirely and relies on a\n",
      "ﬁxed supervision criteria.\n",
      "Detecting Twenty-thousand Classes 7\n",
      "5 Experiments\n",
      "We evaluate Detic on the large-vocabulary object detection dataset LVIS [ 18].\n",
      "We mainly use the open-vocabulary setting proposed by Gu et al. [17], and also\n",
      "report results on the standard LVIS setting. We describe our experiment setup\n",
      "below.\n",
      "LVIS. The LVIS [ 18] dataset has object detection and instance segmentation\n",
      "labels for 1203 classes with 100K images. The classes are divided into three\n",
      "groups - frequent, common, rare based on the number of training images. We\n",
      "refer to this standard LVIS training set as LVIS-all . Following ViLD [ 17], we\n",
      "remove the labels of 337 rare-class from training and consider them as novel\n",
      "classes in testing. We refer to this partial training set with only frequent and\n",
      "common classes as LVIS-base . We report mask mAP which is the oﬃcial metric\n",
      "for LVIS. While our model is developed for box detection, we use a standard\n",
      "class-agnostic mask head [ 20] to produce segmentation masks for boxes. We train\n",
      "the mask head only on detection data.\n",
      "Image-supervised data. We use two sources of image-supervised data: ImageNet-\n",
      "21K [ 10] and Conceptual Captions [ 50]. ImageNet-21K (IN-21K) contains 14M\n",
      "images for 21K classes. For ease of training and evaluation, most of our experi-\n",
      "ments use the 997 classes that overlap with the LVIS vocabulary and denote this\n",
      "subset as IN-L. Conceptual Captions [ 50] (CC) is an image captioning dataset\n",
      "containing 3M images. We extract image labels from the captions using exact\n",
      "text-matching and keep images whose captions mention at least one LVIS class.\n",
      "See Appendix B for results of directly using captions. The resulting dataset\n",
      "contains 1.5M images with 992 LVIS classes. We summarize the datasets used\n",
      "below.\n",
      "Notation Deﬁnition #Images #Classes\n",
      "LVIS-all The original LVIS dataset [18] 100K 1203\n",
      "LVIS-base LVIS without rare-class annotations 100K 866\n",
      "IN-21K The original ImageNet-21K dataset [10] 14M 21k\n",
      "IN-L 997 overlapping IN-21K classes with LVIS 1.2M 997\n",
      "CC Conceptual Captions [50] with LVIS classes 1.5M 992\n",
      "5.1 Implementation details\n",
      "Box-Supervised: a strong LVIS baseline. We ﬁrst establish a strong baseline\n",
      "on LVIS to demonstrate that our improvements are orthogonal to recent advances\n",
      "in object detection. The baseline only uses the supervised bounding box labels. We\n",
      "use the CenterNet2 [ 76] detector with ResNet50 [ 21] backbone. We use Federated\n",
      "Loss [ 76] and repeat factor sampling [ 18]. We use large scale jittering [ 15] with\n",
      "input resolution 640 ×640 and train for a 4 ×(∼48 LVIS epochs) schedule. To\n",
      "show our method is compatible with better pretraining, we use ImageNet-21k\n",
      "pretrained backbone weights [ 48]. As described in §3, we use the CLIP [ 42]\n",
      "embedding as the classiﬁer. Our baseline is 9 .1 mAP higher than the detectron2\n",
      "baseline [ 66] (31.5 vs. 22.4 mAPmask) and trains in a similar time (17 vs. 12\n",
      "hours on 8 V100 GPUs). See Appendix C for more details.\n",
      "8 X. Zhou et al.\n",
      "Resolution change for image-labeled images. ImageNet images are inher-\n",
      "ently smaller and more object-focused than LVIS images [ 73]. In practice, we\n",
      "observe it is important to use smaller image resolution for ImageNet images.\n",
      "Using smaller resolution in addition allows us to increase the batch-size with the\n",
      "same computation. In our implementation, we use 320 ×320 for ImageNet and\n",
      "CC and ablate this in Appendix D.\n",
      "Multi-dataset training. We sample detection and classiﬁcation mini-batches\n",
      "in a 1 : 1 ratio, regardless of the original dataset size. We group images from the\n",
      "same dataset on the same GPU to improve training eﬃciency [77].\n",
      "Training schedules. To shorten the experimental cycle and have a good ini-\n",
      "tialization for prediction-based WSOD losses [ 44,45], we always ﬁrst train a\n",
      "converged base-class-only model (4 ×schedule) and ﬁnetune on it with additional\n",
      "image-labeled data for another 4 ×schedule. We conﬁrm ﬁnetuning the model\n",
      "using only box supervision does not improve the performance. The 4 ×schedule\n",
      "for our joint training consists of ∼24 LVIS epochs plus ∼4.8 ImageNet epochs\n",
      "or∼3.8 CC epochs. Training our ResNet50 model takes ∼22 hours on 8 V100\n",
      "GPUs. The large 21K Swin-B model trains in ∼24 hours on 32 GPUs.\n",
      "5.2 Prediction-based vsnon-prediction-based methods\n",
      "Table 1 shows the results of the box-supervised baseline, existing prediction-\n",
      "based methods, and our proposed non-prediction-based methods. The baseline\n",
      "(Box-Supervised) is trained without access to novel class bounding box labels.\n",
      "It uses the CLIP classiﬁer [ 17] and has open-vocabulary capabilities with 16.3\n",
      "mAP novel. In order to leverage additional image-labeled data like ImageNet or\n",
      "CC, we use prior prediction-based methods or our non-prediction-based method.\n",
      "We compare a few prediction-based methods that assign image labels to\n",
      "proposals based on predictions. Self-training assigns predictions of Box-Supervised\n",
      "as pseudo-labels oﬄine with a ﬁxed score threshold (0 .5). The other prediction-\n",
      "based methods use diﬀerent losses to assign predictions to image labels online.\n",
      "See Appendix E for implementation details. For DLWL [ 44], we implement a\n",
      "simpliﬁed version that does not include bootstrapping and refer to it as DLWL*.\n",
      "Table 1 (third block) shows the results of our non-prediction-based methods\n",
      "in§4. All variants of our proposed simpler method outperform the complex\n",
      "prediction-based counterparts, with both image-supervised datasets. On the novel\n",
      "classes, Detic provides a signiﬁcant gain of ∼4.2 points with ImageNet over the\n",
      "best prediction-based methods.\n",
      "Using non-object centric images from Conceptual Captions. ImageNet\n",
      "images typically have a single large object [ 18]. Thus, our non-prediction-based\n",
      "methods, for example image-box which considers the entire image as a bounding\n",
      "box, are well suited for ImageNet. To test whether our losses work with diﬀerent\n",
      "image distributions with multiple objects, we test it with the Conceptual Captions\n",
      "(CC) dataset. Even on this challenging dataset with multiple objects/labels per\n",
      "image, Detic provides a gain of ∼2.6 points on novel class detection over the\n",
      "best prediction-based methods. This suggests that our simpler Detic method can\n",
      "Detecting Twenty-thousand Classes 9\n",
      "IN-L (object-centric) CC (non object-centric)\n",
      "mAPmaskmAPmask\n",
      "novel mAPmaskmAPmask\n",
      "novel\n",
      "Box-Supervised (baseline) 30.0 ±0.416.3 ±0.7 30.0 ±0.4 16.3 ±0.7\n",
      "Prediction-based methods\n",
      "Self-training [54] 30.3 ±0.015.6 ±0.1 30.1 ±0.2 15.9 ±0.8\n",
      "WSDDN [3] 29.8 ±0.215.6 ±0.3 30.0 ±0.1 16.5 ±0.8\n",
      "DLWL* [44] 30.6 ±0.118.2 ±0.2 29.7 ±0.3 16.9 ±0.6\n",
      "YOLO9000 [45] 31.2 ±0.320.4 ±0.9 29.4 ±0.1 15.9 ±0.6\n",
      "Non-prediction-based methods\n",
      "Detic (Max-object-score) 32.2 ±0.124.4 ±0.3 29.8 ±0.1 18.2 ±0.6\n",
      "Detic (Image-box) 32.4 ±0.123.8 ±0.5 30.9 ±0.119.5 ±0.5\n",
      "Detic (Max-size) 32.4 ±0.124.6 ±0.3 30.9 ±0.219.5 ±0.3\n",
      "Fully-supervised (all classes) 31.1 ±0.425.5 ±0.7 31.1 ±0.4 25.5 ±0.7\n",
      "Table 1: Prediction-based vsnon-prediction-based methods. We show overall\n",
      "and novel-class mAP on open-vocabulary LVIS [ 17] (with 866 base classes and 337 novel\n",
      "classes) with diﬀerent image-labeled datasets (IN-L or CC). The models are trained\n",
      "using our strong baseline §5.1 (top row). This baseline is trained on boxes from the base\n",
      "classes and has non-zero novel-class mAP as it uses the CLIP classiﬁer. All models in the\n",
      "following rows are ﬁnetuned from the baseline model and leverage image-labeled data.\n",
      "We repeat experiments for 3 runs and report mean/ std. All variants of our proposed\n",
      "non-prediction-based losses outperform existing prediction-based counterparts.\n",
      "generalize to diﬀerent types of image-labeled data. Overall, the results from Ta-\n",
      "ble 1 suggest that complex prediction-based methods that overly rely on model\n",
      "prediction scores do not perform well for open-vocabulary detection. Amongst\n",
      "our non-prediction-based variants, the max-size loss consistently performs the\n",
      "best, and is the default for Detic in our following experiments.\n",
      "Why does max-size work? Intuitively, our simpler non-prediction methods\n",
      "outperform the complex prediction-based method by side-stepping a hard as-\n",
      "signment problem. Prediction-based methods rely on strong initial detections to\n",
      "assign image-level labels to predicted boxes. When the initial predictions are reli-\n",
      "able, prediction-based methods are ideal. However, in open-vocabulary scenarios,\n",
      "such strong initial predictions are absent, which explains the limited performance\n",
      "of prediction-based methods. Detic’s simpler assignment does not rely on strong\n",
      "predictions and is more robust under the challenges of open-vocabulary setting.\n",
      "We now study two additional advantages of the Detic max-size variant over\n",
      "prediction-based methods that may contribute to improved performance: 1) the\n",
      "selected max-size proposal can safely cover the target object; 2) the selected\n",
      "max-size proposal is consistent during diﬀerent training iterations.\n",
      "Figure 4 provides typical qualitative examples of the assigned region for the\n",
      "prediction-based method and our max-size variant. On an annotated subset of IN-\n",
      "L, Detic max-size covers 92 .8% target objects, vs. 69 .0% for the prediction-based\n",
      "method. Overall, unlike prediction-based methods, Detic’s simpler assignment\n",
      "yields boxes that are more likely to contain the object. Indeed, Detic may miss\n",
      "10 X. Zhou et al.\n",
      "1/3 iters 2 /3 iters Convergence 1 /3 iters 2 /3 iters ConvergencePred.-based\n",
      " Max-size\n",
      "Fig. 4: Visualization of the assigned boxes during training. We show all boxes\n",
      "with score >0.5 in blue and the assigned (selected) box in red. Top: The prediction-\n",
      "based method selects diﬀerent boxes across training, and the selected box may not\n",
      "cover the objects in the image. Bottom : Our simpler max-size variant selects a box\n",
      "that covers the objects and is more consistent across training.\n",
      "certain objects (especially small objects) or supervise to a loose region. However,\n",
      "in order for Detic to yield a good detector, the selected box need not be perfect,\n",
      "it just needs to 1) provide meaningful training signal (cover the objects and be\n",
      "consistent during training); 2) be ‘more correct’ than the box selected by the\n",
      "prediction-based method. We provide details about our metrics, more quantitative\n",
      "evaluation, and more discussions in Appendix F.\n",
      "5.3 Comparison with a fully-supervised detector\n",
      "In Table 1, compared with the strong baseline Box-Supervised, Detic improves\n",
      "the detection performance by 2 .4 mAP and 8 .3 mAP novel. Thus, Detic with\n",
      "image-level labels leads to strong open-vocabulary detection performance and can\n",
      "provide orthogonal gains to existing open-vocabulary detectors [ 2]. To further\n",
      "understand the open-vocabulary capabilities of Detic, we also report the top-line\n",
      "results trained with box labels for all classes (Table 1 last row). Despite not\n",
      "using box labels for the novel classes, Detic with ImageNet performs favorably\n",
      "compared to the fully-supervised detector. This result also suggests that bounding\n",
      "box annotations may not be required for new classes. Detic combined with large\n",
      "image classiﬁcation datasets is a simple and eﬀective alternative for increasing\n",
      "detector vocabulary.\n",
      "mAPmaskmAPmask\n",
      "novel mAPmask\n",
      "c mAPmask\n",
      "f\n",
      "ViLD-text [17] 24.9 10.1 23.9 32.5\n",
      "ViLD [17] 22.5 16.1 20.0 28.3\n",
      "ViLD-ensemble [17] 25.5 16.6 24.6 30.3\n",
      "Detic 26.8 17.8 26.3 31.6\n",
      "Table 2: Open-vocabulary LVIS compared to ViLD [17]. We train our model\n",
      "using their training settings and architecture (MaskRCNN-ResNet50, training from\n",
      "scratch). We report mask mAP and its breakdown to novel (rare), common, and frequent\n",
      "classes. Variants of ViLD use distillation (ViLD) or ensembling (ViLD-ensemble.). Detic\n",
      "(with IN-L) uses a single model and improves both mAP and mAP novel.\n",
      "Detecting Twenty-thousand Classes 11\n",
      "mAP50box\n",
      "all mAP50box\n",
      "novel mAP50box\n",
      "base\n",
      "Base-only † 39.9 0 49.9\n",
      "Base-only (CLIP) 39.3 1.3 48.7\n",
      "WSDDN [3] † 24.6 20.5 23.4\n",
      "Cap2Det [71] † 20.1 20.3 20.1\n",
      "SB [2] ‡ 24.9 0.31 29.2\n",
      "DELO [78] ‡ 13.0 3.41 13.8\n",
      "PL [43] ‡ 27.9 4.12 35.9\n",
      "OVR-CNN [72] † 39.9 22.8 46.0\n",
      "Detic 45.0 27.8 47.1\n",
      "Table 3: Open-vocabulary COCO [2]. We compare Detic using the same training\n",
      "data and architecture from OVR-CNN [ 72]. We report box mAP at IoU threshold 0.5\n",
      "using Faster R-CNN with ResNet50-C4 backbone. Detic builds upon the CLIP baseline\n",
      "(second row) and shows signiﬁcant improvements over prior work. †: results quoted\n",
      "from OVR-CNN [72] paper or code. ‡: results quoted from the original publications.\n",
      "5.4 Comparison with the state-of-the-art\n",
      "We compare Detic’s open-vocabulary object detectors with state-of-the-art meth-\n",
      "ods on the open-vocabulary LVIS and the open-vocabulary COCO benchmarks.\n",
      "In each case, we strictly follow the architecture and setup from prior work to\n",
      "ensure fair comparisons.\n",
      "Open-vocabulary LVIS. We compare to ViLD [ 17], which ﬁrst uses CLIP\n",
      "embeddings [ 42] for open-vocabulary detection. We strictly follow their training\n",
      "setup and model architecture (Appendix G) and report results in Table 2. Here\n",
      "ViLD-text is exactly our Box-Supervised baseline. Detic provides a gain of 7 .7\n",
      "points on mAP novel. Compared to ViLD-text, ViLD, which uses knowledge distil-\n",
      "lation from the CLIP visual backbone, improves mAP novel at the cost of hurting\n",
      "overall mAP. Ensembling the two models, ViLD-ens provides improvements for\n",
      "both metrics. On the other hand, Detic uses a single model which improves both\n",
      "novel and overall mAP, and outperforms the ViLD ensemble.\n",
      "Open-vocabulary COCO. Next, we compare with prior works on the popular\n",
      "open-vocabulary COCO benchmark [ 2] (see benchmark and implementation\n",
      "details in Appendix H). We strictly follow OVR-CNN [ 72] to use Faster R-\n",
      "CNN with ResNet50-C4 backbone and do not use any improvements from §5.1.\n",
      "Following [ 72], we use COCO captions as the image-supervised data. We extract\n",
      "nouns from the captions and use both the image labels and captions as supervision.\n",
      "Table 3 summarizes our results. As the training set contains only 48 base\n",
      "classes, the base-class only model (second row) yields low mAP on novel classes.\n",
      "Detic improves the baseline and outperforms OVR-CNN [ 72] by a large margin,\n",
      "using exactly the same model, training recipe, and data.\n",
      "Additionally, similar to Table 1, we compare to prior prediction-based methods\n",
      "on the open-vocabulary COCO benchmark in Appendix H. In this setting too,\n",
      "Detic improves over prior work providing signiﬁcant gains on novel class detection\n",
      "and overall detection performance.\n",
      "12 X. Zhou et al.\n",
      "Objects365 [49] OpenImages [28]\n",
      "mAPboxmAPbox\n",
      "rare mAP50boxmAP50box\n",
      "rare\n",
      "Box-Supervised 19.1 14.0 46.2 61.7\n",
      "Detic w. IN-L 21.2 17.8 53.0 67.1\n",
      "Detic w. IN-21k 21.5 20.0 55.2 68.8\n",
      "Dataset-speciﬁc oracles 31.2 22.5 69.9 81.8\n",
      "Table 4: Detecting 21K classes across datasets. We use Detic to train a detector\n",
      "and evaluate it on multiple datasets without retraining . We report the bounding box mAP\n",
      "on Objects365 and OpenImages. Compared to the Box-Supervised baseline (trained\n",
      "on LVIS-all), Detic leverages image-level supervision to train robust detectors. The\n",
      "performance of Detic is 70%-80% of dataset-speciﬁc models (bottom row) that use\n",
      "dataset speciﬁc box labels.\n",
      "5.5 Detecting 21K classes across datasets without ﬁnetuning\n",
      "Next, we train a detector with the full 21K classes of ImageNet. We use our\n",
      "strong recipe with Swin-B [ 37] backbone. In practice, training a classiﬁcation\n",
      "layer of 21K classes is computationally involved.2We adopt a modiﬁed Federated\n",
      "Loss [ 76] that uniformly samples 50 classes from the vocabulary at every iteration.\n",
      "We only compute classiﬁcation scores and back-propagate on the sampled classes.\n",
      "As there are no direct benchmark to evaluate detectors with such large\n",
      "vocabulary, we evaluate our detectors on new datasets without ﬁnetuning . We\n",
      "evaluate on two large-scale object detection datasets: Objects365v2 [ 49] and\n",
      "OpenImages [ 28], both with around 1 .8M training images. We follow LVIS to\n",
      "split1\n",
      "3of classes with the fewest training images as rare classes. Table 4 shows the\n",
      "results. On both datasets, Detic improves the Box-Supervised baseline by a large\n",
      "margin, especially on classes with fewer annotations. Using all the 21k classes\n",
      "2This is more pronounced in detection than classiﬁcation, as the “batch-size” for the\n",
      "classiﬁcation layer is 512 ×image-batch-size, where 512 is #RoIs per image.\n",
      "Fig. 5: Qualitative results of our 21k-class detector. We show random samples\n",
      "from images containing novel classes in OpenImages (top) and Objects365 (bottom)\n",
      "validation sets. We use the CLIP embedding of the corresponding vocabularies. We\n",
      "show LVIS classes in purple and novel classes in green. We use a score threshold of 0 .5\n",
      "and show the most conﬁdent class for each box. Best viewed on screen.\n",
      "Detecting Twenty-thousand Classes 13\n",
      "Box-Supervised Detic\n",
      "Classiﬁer mAPmaskmAPmask\n",
      "novel mAPmaskmAPmask\n",
      "novel\n",
      "*CLIP [42] 30.2 16.4 32.4 24.9\n",
      "Trained 27.4 0 31.7 17.4\n",
      "FastText [24] 27.5 9.0 30.9 19.2\n",
      "OpenCLIP [23] 27.1 8.9 30.7 19.4\n",
      "Table 5: Detic with diﬀerent classiﬁers. We vary the classiﬁer used with Detic\n",
      "and observe that it works well with diﬀerent choices. While CLIP embeddings give the\n",
      "best performance (* indicates our default), all classiﬁers beneﬁt from our Detic.\n",
      "further improves performance owing to the large vocabulary. Our single model\n",
      "signiﬁcantly reduces the gap towards the dataset-speciﬁc oracles and reaches\n",
      "70%-80% of their performance without using the corresponding 1 .8M detection\n",
      "annotations. See Figure 5 for qualitative results.\n",
      "5.6 Ablation studies\n",
      "We now ablate our key components under the open-vocabulary LVIS setting\n",
      "with IN-L as the image-classiﬁcation data. We use our strong training recipe as\n",
      "described in §5.1 for all these experiments.\n",
      "Classiﬁer weights. We study the eﬀect of diﬀerent classiﬁer weights W. While\n",
      "our main open-vocabulary experiments use CLIP [ 42], we show the gain of\n",
      "Detic is independent of CLIP. We train Box-Supervised and Detic with diﬀerent\n",
      "classiﬁers, including a standard random initialized and trained classiﬁer, and\n",
      "other ﬁxed language models [ 23,24] The results are shown in Table 5. By default,\n",
      "a trained classiﬁer cannot recognize novel classes. However, Detic enables novel\n",
      "class recognition ability even in this setting (17 .4 mAP novel for classes without\n",
      "detection labels). Using language models such as FastText [ 24] or an open-source\n",
      "version of CLIP [ 23] leads to better novel class performance. CLIP [ 42] performs\n",
      "the best among them.\n",
      "Eﬀect of Pretraining. Many existing methods use additional data only for\n",
      "pretraining [ 11,72,73], while we use image-labeled data for co-training. We\n",
      "present results of Detic with diﬀerent types of pretraining in Table 6. Detic\n",
      "provides similar gains across diﬀerent types of pretraining, suggesting that our\n",
      "Pretrain data mAPmaskmAPmask\n",
      "novel\n",
      "Box-Supervised IN-1K 26.1 13.6\n",
      "Detic IN-1K 28.8 (+2.7) 21.7 (+8.1)\n",
      "Box-Supervised IN-21K 30.2 16.4\n",
      "Detic IN-21K 32.4 (+2.2) 24.9 (+8.5)\n",
      "Table 6: Detic with diﬀerent pretraining data. Top: our method using ImageNet-\n",
      "1K as pretraining and ImageNet-21K as co-training; Bottom: using ImageNet-21K for\n",
      "both pretraining and co-training. Co-training helps pretraining in both cases.\n",
      "14 X. Zhou et al.\n",
      "Backbone mAPmaskmAPmask\n",
      "r mAPmask\n",
      "c mAPmask\n",
      "f\n",
      "MosaicOS†[73] ResNeXt-101 28.3 21.7 27.3 32.4\n",
      "CenterNet2 [76] ResNeXt-101 34.9 24.6 34.7 42.5\n",
      "AsyncSLL†[19] ResNeSt-269 36.0 27.8 36.7 39.6\n",
      "SeesawLoss [64] ResNeSt-200 37.3 26.4 36.3 43.1\n",
      "Copy-paste [15] EﬃcientNet-B7 38.1 32.1 37.1 41.9\n",
      "Tan et al. [57] ResNeSt-269 38.8 28.5 39.5 42.7\n",
      "Baseline Swin-B 40.7 35.9 40.5 43.1\n",
      "Detic† Swin-B 41.7 41.7 40.8 42.6\n",
      "Table 7: Standard LVIS. We evaluate our baseline (Box-Supervised) and Detic using\n",
      "diﬀerent backbones on the LVIS dataset. We report the mask mAP. We also report\n",
      "prior work on LVIS using large backbone networks (single-scale testing) for references\n",
      "(not for apple-to-apple comparison). †: detectors using additional data. Detic improves\n",
      "over the baseline with increased gains for the rare classes.\n",
      "gains are orthogonal to advances in pretraining. We believe that this is because\n",
      "pretraining improves the overall features, while Detic uses co-training which\n",
      "improves both the features and the classiﬁer.\n",
      "5.7 The standard LVIS benchmark\n",
      "Finally, we evaluate Detic on the standard LVIS benchmark [ 18]. In this setting,\n",
      "the baseline (Box-Supervised) is trained with box and mask labels for all classes\n",
      "while Detic uses additional image-level labels from IN-L. We train Detic with\n",
      "the same recipe in §5.1 and use a strong Swin-B [ 37] backbone and 896 ×896\n",
      "input size. We report the mask mAP across all classes and also split into rare,\n",
      "common, and frequent classes. Notably, Detic achieves 41 .7 mAP and 41 .7 mAP r,\n",
      "closing the gap between the overall mAP and the rare mAP. This suggests Detic\n",
      "eﬀectively uses image-level labels to improve the performance of classes with very\n",
      "few boxes labels. Appendix I provides more comparisons to prior work [ 73] on\n",
      "LVIS. Appendix J shows Detic generalizes to DETR-based [79] detectors.\n",
      "6 Limitations and Conclusions\n",
      "We present Detic which is a simple way to use image supervision in large-\n",
      "vocabulary object detection. While Detic is simpler than prior assignment-based\n",
      "weakly-supervised detection methods, it supervises all image labels to the same\n",
      "region and does not consider overall dataset statistics. We leave incorporating\n",
      "such information for future work. Moreover, open vocabulary generalization\n",
      "has no guarantees on extreme domains. Our experiments show Detic improves\n",
      "large-vocabulary detection with various weak data sources, classiﬁers, detector\n",
      "architectures, and training recipes.\n",
      "Acknowledgments. We thank Bowen Cheng and Ross Girshick for helpful discussions\n",
      "and feedback. This material is in part based upon work supported by the National\n",
      "Science Foundation under Grant No. IIS-1845485 and IIS-2006820. Xingyi is supported\n",
      "by a Facebook PhD Fellowship.\n",
      "Bibliography\n",
      "[1]Arbel´ aez, P., Pont-Tuset, J., Barron, J.T., Marques, F., Malik, J.: Multiscale\n",
      "combinatorial grouping. In: CVPR (2014) 3\n",
      "[2]Bansal, A., Sikka, K., Sharma, G., Chellappa, R., Divakaran, A.: Zero-shot object\n",
      "detection. In: ECCV (2018) 2, 4, 5, 10, 11, 24, 25\n",
      "[3]Bilen, H., Vedaldi, A.: Weakly supervised deep detection networks. In: CVPR\n",
      "(2016) 3, 6, 9, 11, 22, 25\n",
      "[4]Bochkovskiy, A., Wang, C.Y., Liao, H.Y.M.: Yolov4: Optimal speed and accuracy\n",
      "of object detection. arXiv:2004.10934 (2020) 3\n",
      "[5]Cai, Z., Vasconcelos, N.: Cascade r-cnn: Delving into high quality object detection.\n",
      "In: CVPR (2018) 21\n",
      "[6]Chang, N., Yu, Z., Wang, Y.X., Anandkumar, A., Fidler, S., Alvarez, J.M.: Image-\n",
      "level or object-level? a tale of two resampling strategies for long-tailed detection.\n",
      "ICML (2021) 4\n",
      "[7]Chen, L., Yang, T., Zhang, X., Zhang, W., Sun, J.: Points as queries: Weakly\n",
      "semi-supervised object detection by points. In: CVPR (2021) 4\n",
      "[8]Dave, A., Doll´ ar, P., Ramanan, D., Kirillov, A., Girshick, R.: Evaluating large-\n",
      "vocabulary object detectors: The devil is in the details. arXiv:2102.01066 (2021)\n",
      "19, 27\n",
      "[9]Dave, A., Tokmakov, P., Ramanan, D.: Towards segmenting anything that moves.\n",
      "In: ICCVW (2019) 4\n",
      "[10] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale\n",
      "hierarchical image database. In: CVPR (2009) 1, 2, 7\n",
      "[11]Desai, K., Johnson, J.: VirTex: Learning Visual Representations from Textual\n",
      "Annotations. In: CVPR (2021) 13\n",
      "[12]Dong, B., Huang, Z., Guo, Y., Wang, Q., Niu, Z., Zuo, W.: Boosting weakly\n",
      "supervised object detection via learning bounding box adjusters. In: ICCV (2021)\n",
      "3\n",
      "[13] Fang, S., Cao, Y., Wang, X., Chen, K., Lin, D., Zhang, W.: Wssod: A new pipeline\n",
      "for weakly-and semi-supervised object detection. arXiv:2105.11293 (2021) 2, 3\n",
      "[14] Feng, C., Zhong, Y., Huang, W.: Exploring classiﬁcation equilibrium in long-tailed\n",
      "object detection. In: ICCV (2021) 4\n",
      "[15]Ghiasi, G., Cui, Y., Srinivas, A., Qian, R., Lin, T.Y., Cubuk, E.D., Le, Q.V.,\n",
      "Zoph, B.: Simple copy-paste is a strong data augmentation method for instance\n",
      "segmentation. In: CVPR (2021) 7, 14, 21, 24\n",
      "[16]Ghiasi, G., Gu, X., Cui, Y., Lin, T.Y.: Open-vocabulary image segmentation.\n",
      "arXiv:2112.12143 (2021) 4\n",
      "[17] Gu, X., Lin, T.Y., Kuo, W., Cui, Y.: Open-vocabulary object detection via vision\n",
      "and language knowledge distillation. ICLR (2022) 2, 4, 5, 7, 8, 9, 10, 11, 19, 21, 24\n",
      "[18] Gupta, A., Dollar, P., Girshick, R.: LVIS: A dataset for large vocabulary instance\n",
      "segmentation. In: CVPR (2019) 1, 2, 4, 7, 8, 14, 19\n",
      "[19]Han, J., Niu, M., Du, Z., Wei, L., Xie, L., Zhang, X., Tian, Q.: Joint coco and\n",
      "lvis workshop at eccv 2020: Lvis challenge track technical report: Asynchronous\n",
      "semi-supervised learning for large vocabulary instance segmentation (2020) 14\n",
      "[20] He, K., Gkioxari, G., Dollar, P., Girshick, R.: Mask r-cnn. In: ICCV (2017) 4, 7, 24\n",
      "[21] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\n",
      "In: CVPR (2016) 7\n",
      "16 X. Zhou et al.\n",
      "[22]Huang, Z., Zou, Y., Bhagavatula, V., Huang, D.: Comprehensive attention self-\n",
      "distillation for weakly-supervised object detection. NeurIPS (2020) 2, 3\n",
      "[23]Ilharco, G., Wortsman, M., Carlini, N., Taori, R., Dave, A., Shankar, V.,\n",
      "Namkoong, H., Miller, J., Hajishirzi, H., Farhadi, A., Schmidt, L.: Openclip\n",
      "(Jul 2021). https://doi.org/10.5281/zenodo.5143773, https://doi.org/10.5281/\n",
      "zenodo.5143773 13\n",
      "[24] Joulin, A., Grave, E., Bojanowski, P., Douze, M., J´ egou, H., Mikolov, T.: Fasttext.\n",
      "zip: Compressing text classiﬁcation models. arXiv:1612.03651 (2016) 13\n",
      "[25]Kim, D., Lin, T.Y., Angelova, A., Kweon, I.S., Kuo, W.: Learning open-world\n",
      "object proposals without learning to classify. arXiv:2108.06753 (2021) 4\n",
      "[26] Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. ICLR (2015)\n",
      "21\n",
      "[27]Konan, S., Liang, K.J., Yin, L.: Extending one-stage detection with open-world\n",
      "proposals. arXiv:2201.02302 (2022) 4\n",
      "[28]Kuznetsova, A., Rom, H., Alldrin, N., Uijlings, J., Krasin, I., Pont-Tuset, J.,\n",
      "Kamali, S., Popov, S., Malloci, M., Kolesnikov, A., et al.: The open images dataset\n",
      "v4. IJCV (2020) 1, 3, 12\n",
      "[29]Li, B., Weinberger, K.Q., Belongie, S., Koltun, V., Ranftl, R.: Language-driven\n",
      "semantic segmentation. ICLR (2022) 4\n",
      "[30]Li, X., Kan, M., Shan, S., Chen, X.: Weakly supervised object detection with\n",
      "segmentation collaboration. In: ICCV (2019) 3\n",
      "[31]Li, Y., Zhang, J., Huang, K., Zhang, J.: Mixed supervised object detection with\n",
      "robust objectness transfer. TPAMI (2018) 3\n",
      "[32]Li, Y., Wang, T., Kang, B., Tang, S., Wang, C., Li, J., Feng, J.: Overcoming\n",
      "classiﬁer imbalance for long-tail object detection with balanced group softmax. In:\n",
      "CVPR (2020) 4\n",
      "[33]Li, Z., Yao, L., Zhang, X., Wang, X., Kanhere, S., Zhang, H.: Zero-shot object\n",
      "detection with textual descriptions. In: AAAI (2019) 4\n",
      "[34] Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ ar, P.,\n",
      "Zitnick, C.L.: Microsoft coco: Common objects in context. In: ECCV (2014) 1\n",
      "[35] Liu, Y., Zhang, Z., Niu, L., Chen, J., Zhang, L.: Mixed supervised object detection\n",
      "by transferringmask prior and semantic similarity. In: NeurIPS (2021) 3\n",
      "[36]Liu, Y.C., Ma, C.Y., He, Z., Kuo, C.W., Chen, K., Zhang, P., Wu, B., Kira, Z.,\n",
      "Vajda, P.: Unbiased teacher for semi-supervised object detection. ICLR (2021) 2\n",
      "[37]Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin\n",
      "transformer: Hierarchical vision transformer using shifted windows. ICCV (2021)\n",
      "12, 14, 21, 22\n",
      "[38]Maaz, M., Rasheed, H., Khan, S., Khan, F.S., Anwer, R.M., Yang, M.H.: Multi-\n",
      "modal transformers excel at class-agnostic object detection. arXiv:2111.11430\n",
      "(2021) 4\n",
      "[39] Pan, T.Y., Zhang, C., Li, Y., Hu, H., Xuan, D., Changpinyo, S., Gong, B., Chao,\n",
      "W.L.: On model calibration for long-tailed object detection and instance segmen-\n",
      "tation. NeurIPS (2021) 4\n",
      "[40] Pennington, J., Socher, R., Manning, C.D.: Glove: Global vectors for word repre-\n",
      "sentation. In: EMNLP (2014) 4\n",
      "[41]Pinheiro, P.O., Collobert, R.: Weakly supervised semantic segmentation with\n",
      "convolutional networks. In: CVPR (2015) 3\n",
      "[42] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G.,\n",
      "Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from\n",
      "natural language supervision. arXiv:2103.00020 (2021) 4, 5, 7, 11, 13, 19, 20, 21\n",
      "Detecting Twenty-thousand Classes 17\n",
      "[43] Rahman, S., Khan, S., Barnes, N.: Improved visual-semantic alignment for zero-shot\n",
      "object detection. In: AAAI (2020) 4, 11, 25\n",
      "[44] Ramanathan, V., Wang, R., Mahajan, D.: Dlwl: Improving detection for lowshot\n",
      "classes with weakly labelled data. In: CVPR (2020) 2, 3, 6, 8, 9, 22, 23, 25\n",
      "[45] Redmon, J., Farhadi, A.: Yolo9000: better, faster, stronger. In: CVPR (2017) 2, 3,\n",
      "4, 6, 8, 9, 22, 24, 25\n",
      "[46]Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object\n",
      "detection with region proposal networks. NIPS (2015) 4, 5, 25\n",
      "[47]Ren, Z., Yu, Z., Yang, X., Liu, M.Y., Schwing, A.G., Kautz, J.: Ufo2: A uniﬁed\n",
      "framework towards omni-supervised object detection. In: ECCV (2020) 4\n",
      "[48] Ridnik, T., Ben-Baruch, E., Noy, A., Zelnik-Manor, L.: Imagenet-21k pretraining\n",
      "for the masses. In: NeurIPS (2021) 7, 21\n",
      "[49] Shao, S., Li, Z., Zhang, T., Peng, C., Yu, G., Zhang, X., Li, J., Sun, J.: Objects365:\n",
      "A large-scale, high-quality dataset for object detection. In: ICCV (2019) 1, 3, 12\n",
      "[50] Sharma, P., Ding, N., Goodman, S., Soricut, R.: Conceptual captions: A cleaned,\n",
      "hypernymed, image alt-text dataset for automatic image captioning. In: ACL\n",
      "(2018) 7\n",
      "[51] Shen, Y., Ji, R., Wang, Y., Chen, Z., Zheng, F., Huang, F., Wu, Y.: Enabling deep\n",
      "residual networks for weakly supervised object detection. In: ECCV (2020) 3\n",
      "[52] Shen, Y., Ji, R., Wang, Y., Wu, Y., Cao, L.: Cyclic guidance for weakly supervised\n",
      "joint detection and segmentation. In: CVPR (2019) 3\n",
      "[53] Singh, B., Li, H., Sharma, A., Davis, L.S.: R-fcn-3000 at 30fps: Decoupling detection\n",
      "and classiﬁcation. In: CVPR (2018) 4\n",
      "[54]Sohn, K., Zhang, Z., Li, C.L., Zhang, H., Lee, C.Y., Pﬁster, T.: A simple semi-\n",
      "supervised learning framework for object detection. arXiv:2005.04757 (2020) 9,\n",
      "25\n",
      "[55] Tan, J., Lu, X., Zhang, G., Yin, C., Li, Q.: Equalization loss v2: A new gradient\n",
      "balance approach for long-tailed object detection. In: CVPR (2021) 4\n",
      "[56] Tan, J., Wang, C., Li, B., Li, Q., Ouyang, W., Yin, C., Yan, J.: Equalization loss\n",
      "for long-tailed object recognition. In: CVPR (2020) 4\n",
      "[57] Tan, J., Zhang, G., Deng, H., Wang, C., Lu, L., Li, Q., Dai, J.: 1st place solution of\n",
      "lvis challenge 2020: A good box is not a guarantee of a good mask. arXiv:2009.01559\n",
      "(2020) 14\n",
      "[58] Tan, M., Pang, R., Le, Q.V.: Eﬃcientdet: Scalable and eﬃcient object detection.\n",
      "In: CVPR (2020) 21\n",
      "[59] Tang, P., Wang, X., Bai, S., Shen, W., Bai, X., Liu, W., Yuille, A.: Pcl: Proposal\n",
      "cluster learning for weakly supervised object detection. TPAMI (2018) 2, 3\n",
      "[60]Tang, P., Wang, X., Bai, X., Liu, W.: Multiple instance detection network with\n",
      "online instance classiﬁer reﬁnement. In: CVPR (2017) 3\n",
      "[61] Uijlings, J., Popov, S., Ferrari, V.: Revisiting knowledge transfer for training object\n",
      "class detectors. In: CVPR (2018) 3\n",
      "[62] Uijlings, J.R., Van De Sande, K.E., Gevers, T., Smeulders, A.W.: Selective search\n",
      "for object recognition. IJCV (2013) 3\n",
      "[63]Wan, F., Liu, C., Ke, W., Ji, X., Jiao, J., Ye, Q.: C-mil: continuation multiple\n",
      "instance learning for weakly supervised object detection. In: CVPR (2019) 3\n",
      "[64]Wang, J., Zhang, W., Zang, Y., Cao, Y., Pang, J., Gong, T., Chen, K., Liu, Z.,\n",
      "Loy, C.C., Lin, D.: Seesaw loss for long-tailed instance segmentation. In: CVPR\n",
      "(2021) 4, 14\n",
      "[65] Wu, J., Song, L., Wang, T., Zhang, Q., Yuan, J.: Forest r-cnn: Large-vocabulary\n",
      "long-tailed object detection and instance segmentation. In: ACM Multimedia (2020)\n",
      "4\n",
      "18 X. Zhou et al.\n",
      "[66]Wu, Y., Kirillov, A., Massa, F., Lo, W.Y., Girshick, R.: Detectron2. https://\n",
      "github.com/facebookresearch/detectron2 (2019) 7, 21\n",
      "[67] Xu, M., Zhang, Z., Hu, H., Wang, J., Wang, L., Wei, F., Bai, X., Liu, Z.: End-to-end\n",
      "semi-supervised object detection with soft teacher. ICCV (2021) 2\n",
      "[68] Yan, Z., Liang, J., Pan, W., Li, J., Zhang, C.: Weakly-and semi-supervised object\n",
      "detection with expectation-maximization algorithm. arXiv:1702.08740 (2017) 3\n",
      "[69]Yang, H., Wu, H., Chen, H.: Detecting 11k classes: Large scale object detection\n",
      "without ﬁne-grained bounding boxes. In: ICCV (2019) 4\n",
      "[70]Yang, K., Li, D., Dou, Y.: Towards precise end-to-end weakly supervised object\n",
      "detection network. In: ICCV (2019) 3\n",
      "[71] Ye, K., Zhang, M., Kovashka, A., Li, W., Qin, D., Berent, J.: Cap2det: Learning\n",
      "to amplify weak caption supervision for object detection. In: ICCV (2019) 11\n",
      "[72] Zareian, A., Rosa, K.D., Hu, D.H., Chang, S.F.: Open-vocabulary object detection\n",
      "using captions. In: CVPR (2021) 3, 4, 11, 13, 25\n",
      "[73] Zhang, C., Pan, T.Y., Li, Y., Hu, H., Xuan, D., Changpinyo, S., Gong, B., Chao,\n",
      "W.L.: Mosaicos: A simple and eﬀective use of object-centric images for long-tailed\n",
      "object detection. ICCV (2021) 3, 8, 13, 14, 25, 26\n",
      "[74]Zhang, S., Li, Z., Yan, S., He, X., Sun, J.: Distribution alignment: A uniﬁed\n",
      "framework for long-tail visual recognition. In: CVPR (2021) 4\n",
      "[75]Zhong, Y., Wang, J., Peng, J., Zhang, L.: Boosting weakly supervised object\n",
      "detection with progressive knowledge transfer. In: ECCV. Springer (2020) 3\n",
      "[76]Zhou, X., Koltun, V., Kr¨ ahenb¨ uhl, P.: Probabilistic two-stage detection.\n",
      "arXiv:2103.07461 (2021) 4, 7, 12, 14, 21, 26\n",
      "[77]Zhou, X., Koltun, V., Kr¨ ahenb¨ uhl, P.: Simple multi-dataset detection. CVPR\n",
      "(2022) 8\n",
      "[78] Zhu, P., Wang, H., Saligrama, V.: Don’t even look once: Synthesizing features for\n",
      "zero-shot detection. In: CVPR (2020) 11\n",
      "[79] Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: Deformable detr: Deformable\n",
      "transformers for end-to-end object detection. ICLR (2021) 14, 26\n",
      "Detecting Twenty-thousand Classes 19\n",
      "ARr50@100 AR r50@300 AR r50@1k AR50@1k\n",
      "LVIS-all 63.3 76.3 79.7 80.9\n",
      "LVIS-base 62.2 76.2 78.5 81.0\n",
      "(a) Proposal networks trained with (top) and without (bottom) rare classes. We report\n",
      "recalls on rare classes and all classes at IoU threshold 0.5 with diﬀerent number of proposals. Proposal\n",
      "networks trained without rare classes can generalize to rare classes in testing.\n",
      "ARhalf-1st 50@1k AR half-2nd 50@1k\n",
      "LVIS-half-1st 80.8 69.6\n",
      "LVIS-half-2nd 62.9 82.2\n",
      "(b) Proposal networks trained on half of the LVIS classes. We report recalls at IoU threshold\n",
      "0.5 on the other half classes. Proposal networks produce non-trivial recalls on novel classes.\n",
      "Table 8: Proposal network generalization ability evaluation. (a) : Generalize\n",
      "from 866 LVIS base classes to the 337 rare classes; (b): Generalize from uniformly\n",
      "sampled half LVIS classes (601/ 602 classes) to the other half.\n",
      "A Region proposal quality\n",
      "In this section, we show the region proposal network trained on LVIS [ 18] is\n",
      "satisfactory and generalizes well to new classes by default. We experiment under\n",
      "our strong baseline in §5.1. Table 8a shows the proposal recalls with or without\n",
      "rare classes in training. First, we observe the recall gaps between the two models\n",
      "on rare classes are small (79.7 vs. 78.5); second, the gaps between rare classes and\n",
      "all classes are small (79.7 vs. 80.9); third, the absolute recall is relatively high\n",
      "(∼80%, note recall at IoU threshold 0.5 can be translated into oracle mAP-pool [ 8]\n",
      "given perfect classiﬁer and regressor). All observations indicate the proposals have\n",
      "good generalization abilities to new classes even though they are supervised to\n",
      "background during training. We consider the proposal generalization is currently\n",
      "not the performance bottleneck in open-vocabulary detection. This especially the\n",
      "case as modern detectors use an over-suﬃcient number of proposals in testing\n",
      "(1K proposals for <20 objects per image). Our observations are consistent with\n",
      "ViLD [17].\n",
      "We in addition evaluate a more strict setting, where we uniformly split LVIS\n",
      "classes into two halves. I.e., we use classes ID 1 ,3,5,···as the ﬁrst half, and the\n",
      "rest as the second half. These two subsets have completely diﬀerent deﬁnitions\n",
      "of “objects”. We then train a proposal network on each of them, and evaluate\n",
      "on both subsets. As shown in Table 8b, the proposal networks give non-trivial\n",
      "recalls at the complementary other half (69 .6% over 82 .2% percent of the full\n",
      "generalizability). This again supports proposal networks trained on a diverse\n",
      "vocabulary learned a general concept of objects.\n",
      "B Direct captions supervision\n",
      "As we are using a language model CLIP [ 42] as the classiﬁer, our framework can\n",
      "seamlessly incorporate the free-form caption text as image-supervision. Using\n",
      "20 X. Zhou et al.\n",
      "Supervision mAPmaskmAPmask\n",
      "novel\n",
      "Box-Supervised - 30.2 16.4\n",
      "Detic w. CC Image label 31.0 19.8\n",
      "Detic w. CC Caption 30.4 17.4\n",
      "Detic w. CC Both 31.0 21.3\n",
      "mAP50box\n",
      "allmAP50box\n",
      "novel\n",
      "Box-Supervised - 39.3 1.3\n",
      "Detic w. COCO-cap. Image label 44.7 24.1\n",
      "Detic w. COCO-cap. Caption 43.8 21.0\n",
      "Detic w. COCO-cap. Both 45.0 27.8\n",
      "Table 9: Direct caption supervision. Top: Open-vocabulary LVIS with Conceptual\n",
      "Caption as weakly-labeled data; Bottom block: Open-vocabulary COCO with COCO-\n",
      "caption as weakly-labeled data. Directly using caption embeddings as a classiﬁer is\n",
      "helpful on both benchmarks; the improvements are complementary to Detic.\n",
      "the notations in §4, hereDcls={(I,t)i}wheretis a free-form text. In our\n",
      "open-vocabulary detection formulation, text tcan natrually be converted to an\n",
      "embedding by the CLIP [ 42] language encoder L:w=L(t). Given a minibatch\n",
      "ofBsamples{(I,t)i}B\n",
      "i=1, we compose a dynamic classiﬁcation layer by stacking\n",
      "all caption features within the batch ˜W=L({ti}B\n",
      "i=1). For thei-th image in the\n",
      "minibatch, its “classiﬁcation” label is the i-th text, and other texts are negative\n",
      "samples. We use the injected whole image box to extract RoI feature f′\n",
      "ifor image\n",
      "i. We use the same binary cross entropy loss as classifying image labels:\n",
      "Lcap=B∑\n",
      "i=1BCE (˜Wf′\n",
      "i,i)\n",
      "We do not back-propagate into the language encoder.\n",
      "We evaluate the eﬀectiveness of the caption loss in Table 9 on both open-\n",
      "vocabulary LVIS and COCO (see dataset details in Appendix H). We compare\n",
      "individually applying the max-size loss for image labels and the caption loss,\n",
      "and applying both of them. Both image labels and captions can improve both\n",
      "overall mAP and novel class mAP. Combining both losses gives a more signiﬁcant\n",
      "improvement. Our open-vocabulary COCO results in Table 3 uses both the\n",
      "max-size loss and the caption loss.\n",
      "C LVIS baseline details\n",
      "We ﬁrst describe the standard LVIS baseline from the detectron2 model zoo3.\n",
      "This baseline uses ResNet-50 FPN backbone and a 2 ×training schedule (180 k\n",
      "3https://github.com/facebookresearch/detectron2/blob/main/configs/\n",
      "LVISv1-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml\n",
      "Detecting Twenty-thousand Classes 21\n",
      "mAPboxmAPbox\n",
      "rmAPmaskmAPmask\n",
      "r T\n",
      "D2 baseline [66] 22.9 11.3 22.4 11.6 12h\n",
      "+Class-agnostic box&mask 22.3 10.1 21.2 10.1 12h\n",
      "+Federated loss [76] 27.0 20.2 24.6 18.2 12h\n",
      "+CenterNet2 [76] 30.7 22.9 26.8 19.4 13h\n",
      "+LSJ 640×640, 4×sched. [15] 31.0 21.6 27.2 20.1 17h\n",
      "+CLIP classiﬁer [42] 31.5 24.2 28 22.5 17h\n",
      "+Adam optimizer, lr2 e-4 [26] 30.4 23.6 26.9 21.4 17h\n",
      "+IN-21k pretrain [48]* 35.3 28.2 31.5 25.6 17h\n",
      "+Input size 896×896 37.1 29.5 33.2 26.9 25h\n",
      "+Swin-B backbone [37] 45.4 39.9 40.7 35.9 43h\n",
      "*Remove rare class ann.[17] 33.8 17.6 30.2 16.4 17h\n",
      "Table 10: LVIS baseline evolution. First row: the conﬁguration from the detectron2\n",
      "model zoo. The following rows change components one by one. Last row: removing rare\n",
      "classes from the “+IN-21k pretrain*” row. The two gray-ﬁlled rows are the baselines in\n",
      "our main paper, for full LVIS and open-vocabulary LVIS, respectively. We show rough\n",
      "wall-clock training times ( T) on our machine with 8 V100 GPUs in the last column.\n",
      "iterations with batch-size 16)4. Data augmentation includes horizontal ﬂip and\n",
      "random resize short side [640, 800], long side <1333. The baseline uses SGD\n",
      "optimizer with a learning rate 0.02 (dropped by 10 ×at 120kand 160kiteration).\n",
      "The bounding box regression head and the mask head are class-speciﬁc.\n",
      "Table 10 shows the roadmap from the detectron2 baseline to our baseline\n",
      "(§5.1). First, we prepare the model for new classes by making the box and mask\n",
      "heads class-agnostic. This slightly hurts performance. We then use Federated\n",
      "loss [ 76] and upgrade the detector to CenterNet2 [ 76] (i.e., replacing RPN\n",
      "with CenterNet and multiplying proposal score to classiﬁcation score). Both\n",
      "modiﬁcations improve mAP and mAP rsigniﬁcantly, and CenterNet2 slightly\n",
      "increases the training time.\n",
      "Next, we use the EﬃcientDet [ 15,58] style large-scale jittering and train a\n",
      "longer schedule (4 ×). To balance the training time, we also reduce the training\n",
      "image size to 640 ×640 (the testing size is unchanged at 800 ×1333) and\n",
      "increase batch-size to 64 (with the learning rate scaled up to 0 .08). The resulting\n",
      "augmentation and schedule is slightly better than the default multi-scale training,\n",
      "with 30% more training time. A longer schedule is beneﬁcial when using more\n",
      "data, and can be improved by larger resolution.\n",
      "Next, we switch in the CLIP classiﬁer [ 42]. We follow ViLD [ 17] to L2\n",
      "normalize the embedding and RoI feature before dot-product. Note CenterNet2\n",
      "uses a cascade classiﬁer [ 5]. We use CLIP for all of them. Using CLIP classiﬁer\n",
      "improves rare class mAP.\n",
      "Finally, we use an ImageNet-21k pretrained ResNet-50 model from Ridnik\n",
      "et al. [48]. We remark the ImageNet-21k pretrained model requires using Adam\n",
      "optimizer (with learning rate 2 e-4). Combing all the improvements results in\n",
      "4We are aware diﬀerent projects use diﬀerent notations of a 1 ×schedule. In this paper\n",
      "we always refer 1×schedule to 16×90kimages\n",
      "22 X. Zhou et al.\n",
      "Ratio Size mAPmaskmAPmask\n",
      "novel\n",
      "Bos-Supervised 1: 0 - 30.2 16.4\n",
      "Detic w. IN-L 1: 1 640 30.9 23.3\n",
      "Detic w. IN-L 1: 1 320 32.0 24.0\n",
      "Detic w. IN-L 1: 4 640 31.1 23.5\n",
      "Detic w. IN-L 1: 4 320 32.4 24.9\n",
      "Detic w. CC 1: 1 640 30.8 21.6\n",
      "Detic w. CC 1: 1 320 30.8 21.5\n",
      "Detic w. CC 1: 4 640 30.7 21.0\n",
      "Detic w. CC 1: 4 320 31.1 21.8\n",
      "Table 11: Ablations of the resolution change. We report mask mAP on the open-\n",
      "vocabulary LVIS following the setting of Table 1. Top: ImageNet as the image-labeled\n",
      "data. Bottom: CC as the image-labeled data.\n",
      "35.3 mAPboxand 31.5 mAPmask, and trains in a favorable time (17h on 8 V100\n",
      "GPUs). We use this model as our baseline in the main paper.\n",
      "Increasing the training resolution or using a larger backbone [ 37] can further\n",
      "increase performance signiﬁcantly, at a cost of longer training time. We use the\n",
      "large models only when compared to the state-of-the-art models.\n",
      "D Resolution change for classiﬁcation data\n",
      "Table 11 ablates the resolution change in §5.1. Using a smaller input resolution\n",
      "improves∼1 point for both mAP and mAP novel with ImageNet, but does not\n",
      "impact much with CC. Using more batches for the weak datasets is slightly better\n",
      "than a 1 : 1 ratio.\n",
      "E Prediction-based losses implementation details\n",
      "Following the notations in §4, we implement the prediction-based weakly-\n",
      "supervised detection losses as below:\n",
      "WSDDN [3] learns a soft weight on the proposals to weight-sum the proposal\n",
      "classiﬁcation scores into a single image classiﬁcation score:\n",
      "LWSDDN =BCE (∑\n",
      "j(softmax( W′F)j∗Sj),c)\n",
      "where W′is a learnable network parameter.\n",
      "Predicted [45] selects the proposal with the max predicted score on class c:\n",
      "LPredicted =BCE (Sj,c),j= argmaxjSjc\n",
      "DLWL* [44] ﬁrst runs a clustering algorithm with IoU threshold 0.5. Let Jbe\n",
      "the set of peaks of each cluster (i.e., the proposal within the cluster and has the\n",
      "Detecting Twenty-thousand Classes 23\n",
      "max predicted score on class c), We then select the top Nc= 3 peaks with the\n",
      "highest prediction scores on class c.\n",
      "LDLWL* =1\n",
      "NcNc∑\n",
      "t=1BCE (Sjt,c),\n",
      "jt= argmaxj∈J,j̸={j1,...,jt−1}Sjc\n",
      "The original DLWL [ 44] in addition upgrades Susing an IoU-based assignment\n",
      "matrix from self-training and bootstrapping (See their Section 3.2). In our\n",
      "implementation, we did not include this part, as our goal is to only compare the\n",
      "training losses.\n",
      "F More comparison between prediction-based and\n",
      "non-prediction-based methods\n",
      "Our non-prediction-based losses perform signiﬁcantly better than prediction-\n",
      "based losses as is shown in Table 1. In this section, we take the max-size loss and\n",
      "the predicted-loss as the representitives and conduct more detailed comparisons\n",
      "between them. A straightforward reason is that the predicted loss requires a\n",
      "good initial prediction to guide the pseudo-label-based training. However in the\n",
      "open-vocabulary detection setting the initial predictions are inherently ﬂawed. To\n",
      "verify this, in Table 12a, we show both improving the backbone and including\n",
      "rare classes in training can narrow the gap. However in the current performance\n",
      "regime, our max-size loss performs better.\n",
      "We highlight two additional advantages of the max-size loss that may con-\n",
      "tribute to the good performance: (1) the max-size loss is a safe approximation\n",
      "of object regions; (2) the max-size loss is consistent during training. Figure 4\n",
      "provides qualitative examples of the assigned region for the predicted loss and\n",
      "the max-size loss. First, we observe that while being coarse at the boundary, the\n",
      "max-size loss can cover the target object in most cases. Second, the assigned\n",
      "regions of the predicted loss are usually diﬀerent across training iterations, es-\n",
      "pecially in the early phase where the model predictions are unstable. On the\n",
      "contrary, max-size loss supervises consistent regions across training iterations.\n",
      "Table 12b quantitatively evaluates these two properties. We use the ground\n",
      "truth box annotation in the full COCO detection dataset and a subset of ImageNet\n",
      "with bounding box annotation5to evaluate the cover rate. We deﬁne cover rate\n",
      "as the ratio of image labels whose ground-truth box has >0.5 intersection-over-\n",
      "area with the assigned region. We deﬁne the consistency metric as the average\n",
      "assigned-region IoU of the same image between the 1 /2 schedule and the ﬁnal\n",
      "schedule. Table 12b shows max-size loss is more favorable than predicted loss\n",
      "on these two metrics. However we highlight that these two metrics alone do not\n",
      "always correlate to the ﬁnal performance, as the image-box loss is perfect on\n",
      "both metrics but underperforms max-size loss.\n",
      "5https://image-net.org/download-bboxes.php . 213K of the 1.2M IN-L images have\n",
      "bounding box annotations.\n",
      "24 X. Zhou et al.\n",
      "Dataset Backbone mAPmaskmAPmask\n",
      "novel\n",
      "Box-Supervised 30.2 16.4\n",
      "Predicted LVIS-base Res50 31.2 20.4\n",
      "Max-size 32.4 (+1.2) 24.6 (+4.2)\n",
      "Box-Supervised 38.4 21.9\n",
      "Predicted LVIS-base SwinB 40.0 31.7\n",
      "Max-size 40.7 (+0.7) 33.8 (+2.1)\n",
      "Box-Supervised 31.5 25.6\n",
      "Predicted LVIS-all Res50 32.5 28.4\n",
      "Max-size 33.2 (+0.7) 29.7 (+1.3)\n",
      "Box-Supervised 40.7 35.9\n",
      "Predicted LVIS-all SwinB 40.6 39.8\n",
      "Max-size 41.3 (+0.7) 40.9 (+1.1)\n",
      "(a) Predicted loss and max-size loss with diﬀerent prediction qualities. We show the mask\n",
      "mAP of the box-supervised baseline, Predicted loss [45], and our max-size loss. We show the delta\n",
      "between max-size loss and predicted loss in green. Improving the backbone and including rare classes\n",
      "in training can both narrow the gap. Max-size consistently performs better.\n",
      "Cover rate Consistency\n",
      "IN-L COCO IN-L CC COCO\n",
      "Predicted 69.0 73.8 71.5 30.0 57.7\n",
      "Max-size 92.8 80.0 87.9 73.0 62.8\n",
      "(b) Assigned proposal cover rate and consistency. Left: ratio of assigned proposal covering the\n",
      "ground truth both. We evaluate on an ImageNet subset that has box ground truth and the annotated\n",
      "COCO training set; Right: average assigned bounding box IoU of between the ﬁnal model and the\n",
      "half-schedule model.\n",
      "Table 12: Comparison between predicted loss and and max-size loss. (a) :\n",
      "comparison under diﬀerent baselines. (b): comparison in customized metrics.\n",
      "G ViLD baseline details\n",
      "The baseline in ViLD [ 17] is very diﬀerent from detectron2. They use MaskRCNN\n",
      "detector [ 20] with Res50-FPN backbone, but trains the network from scratch\n",
      "without ImageNet pretraining. They use large-scale jittering [ 15] with input\n",
      "resolution 1024×1024 and train a 32 ×schedule. The optimizer is SGD with\n",
      "batch size 256 and learning rate 0 .32. We ﬁrst reproduce their baselines (both the\n",
      "oracle detector and ViLD-text) under the same setting. We observe half of their\n",
      "schedule (16×) is suﬃcient to closely match their numbers. The half training\n",
      "schedule takes 4 days on 4 nodes (each with 8 V100 GPUs). We then ﬁnetune\n",
      "another 16×schedule using ImageNet data with our max-size loss.\n",
      "H Open-vocabulary COCO benchmark details\n",
      "Open-vocabulary COCO is proposed by Bansal et al. [ 2]. They manually select 48\n",
      "classes from the 80 COCO classes as base classes, and 17 classes as novel classes.\n",
      "Detecting Twenty-thousand Classes 25\n",
      "mAP50box\n",
      "all mAP50box\n",
      "novel\n",
      "Box-Supervised (base cls) 39.3 1.3\n",
      "Self-training [54] 39.5 1.8\n",
      "WSDDN [3] 39.9 5.9\n",
      "DLWL* [44] 42.9 19.6\n",
      "Predicted [45] 41.9 18.7\n",
      "Detic (Max-object-score) 43.3 20.4\n",
      "Detic (Image-box) 43.4 21.0\n",
      "Detic (Max-size) 44.7 24.1\n",
      "Box-Supervised (all cls) 54.9 60.0\n",
      "Table 13: Diﬀerent ways to use image supervision on open-vocabulary\n",
      "COCO. The models are trained using the OVR-CNN [ 72] recipe with ResNet50-\n",
      "C4 [2] backbone. We follow setups in Table 1. The observations are consistent with\n",
      "LVIS.\n",
      "The training set is the same as the full COCO, but only images containing at least\n",
      "one base class are used. During testing, we report results under the “generalized\n",
      "zero-shot detection” setting [2], where all COCO validation images are used.\n",
      "We strictly follow the literatures [ 2,43,72] to use FasterRCNN [ 46] with\n",
      "ResNet50-C4 backbone and the 1 ×training schedule (90 kiterations). We use\n",
      "horizontal ﬂip as the only data augmentation in training and keep the input\n",
      "resolution ﬁxed to 800 ×1333 in both training and testing. We use SGD optimizer\n",
      "with a learning rate 0 .02 (dropped by 10 ×at 60kand 80kiteration) and batch\n",
      "size 16. The evaluation metric on open-vocabulary COCO is box mAP at IoU\n",
      "threshold 0.5. Our reproduced baseline matches OVR-CNN [ 72]. Our model is\n",
      "ﬁnetuned on the baseline model with another 1 ×schedule. We sample detection\n",
      "data and image-supervised data in a 1 : 1 ratio.\n",
      "Table 13 repeats the experiments in Table 1 on open-vocabulary COCO.\n",
      "The observations are consistent: our proposed non-prediction-based methods\n",
      "outperform existing prediction-based counterparts, and the max-size loss performs\n",
      "the best among our variants.\n",
      "I Compare to MosaicOS [73]\n",
      "MosaicOS [ 73] ﬁrst uses image-level annotations to improve LVIS detectors. We\n",
      "compare to MosaicOS [ 73] by strictly following their baseline setup (without any\n",
      "improvements in §5.1). The detailed hyper-parameters follow the detectron2\n",
      "baseline as described in Appendix C. We ﬁnetune on the Box-supervised model\n",
      "with an additional 2 ×schedule with Adam optimizer. Table 14 shows our\n",
      "re-trained baseline exactly matches their reported results from the paper. Our\n",
      "method is developed based on the CLIP classiﬁer, and we also report our baseline\n",
      "with CLIP. The baseline has slightly lower mAP and higher mAP r. MosaicOS\n",
      "uses IN-L and additional web-search images as image-supervised data. Detic\n",
      "26 X. Zhou et al.\n",
      "mAPmaskmAPmask\n",
      "r\n",
      "Box-Supervised [73] 22.6 12.3\n",
      "MosaicOS [73] 24.5 (+1.9) 18.3 (+6.0)\n",
      "Box-Supervised (Reproduced) 22.6 12.3\n",
      "Detic (default classiﬁer) 25.1 (+2.5) 18.6 (+6.3)\n",
      "Box-Supervised (CLIP classiﬁer) 22.3 14.1\n",
      "Detic (CLIP classiﬁer) 24.9 (+2.6) 20.7 (+6.5)\n",
      "Table 14: Standard LVIS compared to MosiacOS [73]. Top block: results quoted\n",
      "from MosiacOS paper; Middle block: Detic with the default random intialized and\n",
      "trained classiﬁer; Bottom block: Detic with CLIP classiﬁer.\n",
      "mAPboxmAPbox\n",
      "rmAPbox\n",
      "cmAPbox\n",
      "f\n",
      "Box-Supervised 31.7 21.4 30.7 37.5\n",
      "Detic 32.5 26.2 31.3 36.6\n",
      "Table 15: Detic applied to Deformable-DETR [79]. We report Box mAP on full\n",
      "LVIS. Our method improves Deformable-DETR.\n",
      "outperforms MosaicOS [ 73] in mAP and mAP r, without using their multi-stage\n",
      "training and mosaic augmentation. Our relative improvements over the baseline\n",
      "are slightly higher than MosiacOS [ 73]. We highlight our training framework is\n",
      "simpler and we use less additional training data (Google-searched images).\n",
      "J Generalization to Deformable-DETR.\n",
      "We apply Detic to the recent Transformer based Deformable-DETR [ 79] to study\n",
      "its generalization. We use their default training recipe, Federated Loss [ 76] and\n",
      "train for a 4×schedule (∼48 LVIS epochs). We apply the image supervision to\n",
      "the query from the encoder with the max predicted size. Table 15 shows that\n",
      "Detic improves over the baseline (+0 .8 mAP and +4 .8 mAP r) and generalizes to\n",
      "Transformer based detectors.\n",
      "mAPmaskmAPmask\n",
      "IN-L mAPmask\n",
      "non-IN-L\n",
      "Box-Supervised 30.2 30.6 27.6\n",
      "Max-size 32.4 33.5 28.1\n",
      "mAPmaskmAPmask\n",
      "CC mAPmask\n",
      "non-CC\n",
      "Box-Supervised 30.2 30.1 29.5\n",
      "Max-size 30.9 31.7 28.6\n",
      "Table 16: mAP breakdown into classes with and without image labels. Top:\n",
      "Detic trained on ImageNet. Bottom: Detic trained on CC. Most of the improvements are\n",
      "from classes with image-level labels. On ImageNet Detic also improves classes without\n",
      "image labels thanks to the CLIP classiﬁer.\n",
      "Detecting Twenty-thousand Classes 27\n",
      "Datasets mAPboxmAPbox\n",
      "novel mAPFixedmAPFixed\n",
      "novel\n",
      "Box-Supervised 30.2 16.4 31.2 18.2\n",
      "Detic 32.4 (+2.2) 24.9 (+8.5) 33.4 (+2.3) 26.7 (+8.5)\n",
      "Table 17: mAPFixedevaluation . Middle: the original box mAP metric used in the\n",
      "main paper. Right: the new box mAPFixmetric. Our improvements are consistent under\n",
      "the new metric.\n",
      "K Improvements breakdown to classes\n",
      "Table 16 shows mAP breakdown into classes with and without image labels for\n",
      "both the Box-Supervised baseline and Detic. As expected, most of the improve-\n",
      "ments are from classes with image-level labels. On ImageNet, Detic also improves\n",
      "classes without image labels thanks to the CLIP classiﬁer which leverages inter-\n",
      "class relations.\n",
      "L mAPFixedevaluation\n",
      "Table 17 compares our improvements under the new mAPﬁxproposed in Dave et\n",
      "al.[8]. Our improvements are consistent under the new metric.\n",
      "M Image Attributions\n",
      "License for the images from OpenImages in Figure 5:\n",
      "–“Oyster”: Photo by The Local People Photo Archive (CC BY 2.0)\n",
      "–“Cheetah”: Photo by Michael Gil (CC BY 2.0)\n",
      "–“Harbor seal”: Photo by Alden Chadwick (CC BY 2.0)\n",
      "–“Dinosaur”: Photo by Paxson Woelber (CC BY 2.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = co.chat(\n",
    "\tmessage=prompt, \n",
    "\tmodel=\"command-r\", \n",
    "\ttemperature=0.1,\n",
    "    max_tokens=1000,\n",
    ")\n",
    "\n",
    "answer = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_citing_context = answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Detic model mentioned in the paragraph is a simple way to use image supervision in large vocabulary object detection. It trains the classifiers of a detector on image classification data, which enables the detector to identify tens of thousands of concepts. Unlike other models, Detic does not require complex assignment schemes. The novel contributions of the Detic model are as follows:\n",
      "1. It identifies issues with existing weakly-supervised detection techniques in the open-vocabulary setting and provides a simpler alternative.\n",
      "2. The model's family of losses significantly improves detection performance on novel classes and matches the supervised performance upper bound.\n",
      "3. The detector trained with Detic generalizes well to new datasets and vocabularies without finetuning. \n",
      "4. The method is simple to implement and ready to use for real-world open-vocabulary detection.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answering the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original response with no extra-context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = co.chat(\n",
    "\tmessage=question, \n",
    "\tmodel=\"command-r\", \n",
    "\ttemperature=0.1,\n",
    "    max_tokens=1000,\n",
    ")\n",
    "\n",
    "answer = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Detic model refers to the work titled \"Detic: Online Instance Selection for Weakly Supervised Detection Training\" which proposes a novel online instance selection method for weakly-supervised object detection. The method uses a max-size loss function, among other loss functions, to improve the training process. \n",
      "\n",
      "The contributions of the Detic model are as follows:\n",
      "1. Online Instance Selection: The model selects the most informative instances during training by proposing an online instance selection strategy. This strategy helps in focusing on hard, uncertain, and diverse instances, which leads to more effective training.\n",
      "2. Max-Size Loss Function: Detic introduces the concept of max-size loss, which encourages the detector to assign larger bounding boxes to positive instances. This loss function prioritizes the selection of larger objects, leading to improved detection performance.\n",
      "3. Efficiency: The online instance selection process is computationally efficient, making it practical for large-scale datasets. It avoids the need for multiple iterations over the data, making the training process faster.\n",
      "4. Flexibility: The Detic method is designed to be flexible and can be incorporated into various weakly-supervised detection frameworks. \n",
      "\n",
      "Overall, the Detic model improves the object detection process in the context of weak supervision by selectively focusing on valuable instances and utilizing a unique loss function.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response with the citing context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Citiation Context:\n",
      "The Detic model mentioned in the paragraph is a simple way to use image supervision in large vocabulary object detection. It trains the classifiers of a detector on image classification data, which enables the detector to identify tens of thousands of concepts. Unlike other models, Detic does not require complex assignment schemes. The novel contributions of the Detic model are as follows:\n",
      "1. It identifies issues with existing weakly-supervised detection techniques in the open-vocabulary setting and provides a simpler alternative.\n",
      "2. The model's family of losses significantly improves detection performance on novel classes and matches the supervised performance upper bound.\n",
      "3. The detector trained with Detic generalizes well to new datasets and vocabularies without finetuning. \n",
      "4. The method is simple to implement and ready to use for real-world open-vocabulary detection.\n",
      "\n",
      "Question:\n",
      "\n",
      "Similar to DECOLA Phase 2, we self-\n",
      "train baseline on weakly-labeled data. For the self-training\n",
      "algorithm, we use online self-training with max-size loss\n",
      "from Detic [74] as baseline comparison (baseline + self-\n",
      "train) to DECOLA Phase 2. We tested max-size and max-\n",
      "score losses from Detic [74] (online pseudo-labeling) as well\n",
      "as offline pseudo-labeling similar to DECOLA , and max-size\n",
      "loss consistently performed the best.\n",
      "\n",
      "What is the Detic model mentioned in this paragraph? What are the novel contributions of the Detic model?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_question = f\"\"\"\n",
    "Citation Context:\n",
    "{relevant_citing_context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "print(new_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = co.chat(\n",
    "\tmessage=new_question, \n",
    "\tmodel=\"command-r\", \n",
    "\ttemperature=0.1,\n",
    "    max_tokens=1000,\n",
    ")\n",
    "\n",
    "answer = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Detic model is a novel approach to large vocabulary object detection that uses image supervision. The model's key contributions are:\n",
      "\n",
      "1. It addresses issues with current open-vocabulary detection techniques by offering a straightforward alternative.\n",
      "2. The family of losses in the Detic model enhances detection performance on novel classes and reaches the supervised performance upper limit.\n",
      "3. The Detic model generalizes well to new datasets and vocabs without the need for fine-tuning.\n",
      "4. The method is simple to implement and use in the real world for open-vocabulary detection.\n",
      "\n",
      "I hope this helps you understand the Detic model and its contributions. Let me know if you have any other questions!\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
